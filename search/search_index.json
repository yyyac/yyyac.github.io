{"config":{"lang":["zh","en","ja"],"separator":"[\\s\\u200b\\u3000\\-\u3001\u3002\uff0c\uff0e\uff1f\uff01\uff1b]+","pipeline":["stemmer"]},"docs":[{"location":"","title":"\u70b9\u51fb\u4e0a\u8ff0\u6807\u9898","text":""},{"location":"ALGORITHM/","title":"\u7b97\u6cd5\u5237\u9898","text":"<p>\u70b9\u51fb\u5de6\u4fa7\u6253\u5f00\u76ee\u5f55</p>"},{"location":"ALGORITHM/Reflections/array/","title":"\u6570\u7ec4","text":""},{"location":"ALGORITHM/Reflections/array/#_2","title":"\u4e8c\u5206\u67e5\u627e","text":"<p>\u4e8c\u5206\u662f\u5728\u6709\u5e8f\u6570\u7ec4\u6216\u8005\u6709\u5e8f\u533a\u95f4\u4e0a\u67e5\u627e\u7684\u9ad8\u6548\u7b97\u6cd5\u3002\u53ef\u4ee5\u91c7\u7528\u7ea2\u84dd\u8fb9\u754c\u6cd5\u6c42\u89e3\uff0c\u57fa\u672c\u601d\u60f3\u4e3a\uff1a</p> <ul> <li>\u6839\u636e\u67e5\u627e\u5143\u7d20\uff0c\u5c06\u6570\u7ec4/\u533a\u95f4\u5206\u4e3a\u4e24\u90e8\u5206\uff08\u7ea2\u84dd\u533a\u95f4\uff09\u3002</li> <li>\u5224\u65ad mid \u5143\u7d20\u662f\u5426\u4e3a\u84dd\u8272\uff0c\u66f4\u65b0\u7ea2\u84dd\u533a\u95f4</li> <li>\u6839\u636e\u9898\u610f\u6700\u7ec8\u8fd4\u56de\u7ea2\u84dd\u8fb9\u754c</li> </ul> <p>\u9700\u8981\u5728\u67e5\u627e\u524d\u5224\u65ad\u82e5 <code>a[0] &gt; x || a[n - 1] &lt; x</code>\uff0c\u5219\u76f4\u63a5\u8fd4\u56de <code>-1</code>\uff1b\u5047\u8bbe\u5b58\u5728\u5e8f\u5217 <code>[1,4,6,7,8]</code>\uff0c\u67e5\u627e\u5143\u7d20\u4e3a <code>0</code>\u3002\u521d\u59cb <code>l = -1, r = 5, mid = 2</code>\uff0c<code>q[mid]</code> \u6c38\u8fdc\u5927\u4e8e x\uff0c\u6545 <code>l</code> \u4e0d\u4f1a\u66f4\u65b0\uff0c\u4f1a\u53d1\u751f\u6570\u7ec4\u8d8a\u754c\uff0c\u6240\u4ee5\u8981\u63d0\u524d\u5904\u7406\u3002\u5176\u4e2d L \u6c38\u8fdc\u6307\u5411\u84dd\u8272\u533a\u95f4\uff0cR \u6c38\u8fdc\u6307\u5411\u7ea2\u8272\u533a\u95f4\uff0c\u6240\u4ee5\u521d\u59cb\u5316\u65f6 <code>l = -1, r = n</code>\u3002</p> <p>\u7b97\u6cd5\u6a21\u677f</p> C++<pre><code>int find(int a[], int n, int x)\n{\n    if(a[0] &gt; x || a[n - 1] &lt; x)\n        return -1;\n    int l = -1, r = n;  //\n    while(l + 1 != r)\n    {\n        int mid = (l + r) / 2;\n        if(IsBlue(mid)) l = mid;\n        else r = mid;\n    }\n    return l or r;\n}\n</code></pre>"},{"location":"ALGORITHM/Reflections/array/#_3","title":"\u4e8c\u5206\u67e5\u627e","text":"C++<pre><code>class Solution {\npublic:\n    int search(vector&lt;int&gt;&amp; nums, int target) {\n        int n = nums.size() - 1;\n        if(nums[0] &gt; target || nums[n] &lt; target)\n            return -1;\n        int l = -1, r = n + 1;\n        while(l + 1 != r)\n        {\n            int mid = (l + r) / 2;\n            if(nums[mid] &lt;  target) l = mid;\n            else r = mid;\n        }\n        if(nums[r] == target) return r;\n        else return -1;\n    }\n};\n</code></pre>"},{"location":"ALGORITHM/Reflections/array/#_4","title":"\u641c\u7d22\u63d2\u5165\u4f4d\u7f6e","text":"C++<pre><code>class Solution {\npublic:\n    int searchInsert(vector&lt;int&gt;&amp; nums, int x) {\n        int n = nums.size() - 1;\n        if(nums[0] &gt; x) return 0;\n        if(nums[n] &lt; x) return n + 1;\n        int l = -1, r = n + 1;\n        while(l + 1 != r)\n        {\n            int mid = (l + r) / 2;\n            if(nums[mid] &lt; x) l = mid;\n            else r = mid;\n        }\n        return r;\n    }\n};\n</code></pre>"},{"location":"ALGORITHM/Reflections/array/#_5","title":"\u5728\u6392\u5e8f\u6570\u7ec4\u4e2d\u67e5\u627e\u5143\u7d20\u7684\u7b2c\u4e00\u4e2a\u548c\u6700\u540e\u4e00\u4e2a\u4f4d\u7f6e","text":"C++<pre><code>class Solution {\npublic:\n    int find1(vector&lt;int&gt;&amp; a, int x)\n    {\n        int n = a.size();\n        int l = -1, r = n;\n        while(l + 1 != r)\n        {\n            int mid = (l + r) / 2;\n            if(a[mid] &lt; x)\n                l = mid;\n            else\n                r = mid;\n        }\n        return r;\n    }\n    int find2(vector&lt;int&gt;&amp; a, int x)\n    {\n        int n = a.size();\n    int l = -1, r = n;\n    while(l + 1 != r)\n    {\n        int mid = (l + r) / 2;\n        if(a[mid] &lt;= x)\n            l = mid;\n        else\n            r = mid;\n    }\n    return l;\n    }\n    vector&lt;int&gt; searchRange(vector&lt;int&gt;&amp; nums, int x) {\n        int n = nums.size() - 1;\n        if(n &lt; 0 || nums[0] &gt; x || nums[n] &lt; x)\n            return vector&lt;int&gt; {-1,-1};\n        int res1 = find1(nums, x);\n        int res2 = find2(nums,x);\n        if(nums[res1] != x)\n            return vector&lt;int&gt; {-1,-1};\n        return vector&lt;int&gt;{res1, res2};\n\n\n\n    }\n};\n</code></pre>"},{"location":"ALGORITHM/Reflections/array/#x","title":"x \u7684\u5e73\u65b9\u6839","text":"C++<pre><code>class Solution {\npublic:\n    int mySqrt(int x) {\n        if(x == 0) return 0;\n        if(x == 1) return 1;\n        int l = -1, r = x;\n        while (l + 1 != r)\n        {\n            long long mid = (l + r) / 2;\n            if (mid * mid &lt;= x)\n                l = mid;\n            else\n                r = mid;\n        }\n        return l;\n    }\n};\n</code></pre>"},{"location":"ALGORITHM/Reflections/array/#_6","title":"\u79fb\u9664\u5143\u7d20","text":""},{"location":"ALGORITHM/Reflections/array/#_7","title":"\u5220\u9664\u6709\u5e8f\u6570\u7ec4\u4e2d\u7684\u91cd\u590d\u9879","text":"<p>Note</p> <ul> <li>\u521d\u59cb\u5316 <code>k = 1</code>\uff0c\u8868\u793a\u4fdd\u7559\u5143\u7d20\u8981\u586b\u5165\u7684\u4e0b\u6807\u3002</li> <li>\u4ece <code>i = 1</code> \u5f00\u59cb\u904d\u5386\u3002</li> <li>\u5982\u679c <code>nums[i] == nums[i - 1]</code>\uff0c\u5219 <code>nums[i]</code> \u662f\u91cd\u590d\u9879\uff0c\u4e0d\u4fdd\u7559\u3002</li> <li>\u5982\u679c <code>nums[i] != nums[i - 1]</code>\uff0c\u5219 <code>nums[i]</code> \u4e0d\u662f\u91cd\u590d\u9879\uff0c\u4fdd\u7559\uff0c\u586b\u5165 <code>nums[k]</code> \u4e2d\uff0c\u7136\u540e k++\u3002</li> <li>\u904d\u5386\u7ed3\u675f\u540e\uff0ck \u5c31\u662f nums \u4e2d\u7684\u552f\u4e00\u5143\u7d20\u7684\u6570\u91cf</li> </ul> C++<pre><code>class Solution {\npublic:\n    int removeDuplicates(vector&lt;int&gt;&amp; nums) {\n        int k = 1;\n        for(int i = 1; i &lt; nums.size(); i ++)\n        {\n            if(nums[i] != nums[i - 1])\n                nums[k ++] = nums[i];\n        }\n        return k;\n    }\n};\n</code></pre>"},{"location":"ALGORITHM/Reflections/array/#_8","title":"\u79fb\u52a8\u96f6","text":"<p>Note</p> <p>\u4ece\u5de6\u5230\u53f3\u904d\u5386 <code>nums[i]</code>\uff0c\u540c\u65f6\u7ef4\u62a4\u53e6\u4e00\u4e2a\u4e0b\u6807 \\(i_0\\)\uff0c\u4fdd\u8bc1\u4e0b\u6807\u533a\u95f4 \\([i_0,i-1]\\) \u90fd\u662f 0\uff0c\u4e14\u628a \\(i_0\\) \u6307\u5411\u6700\u5de6\u8fb9\u7684 \\(0\\)\u3002 \u6bcf\u6b21\u9047\u5230 \\(nums[i] != 0\\) \u7684\u60c5\u51b5\uff0c\u5c31\u628a \\(nums[i]\\) \u4e0e \\(nums[i_0]\\) \u4ea4\u6362\u3002\u4ea4\u6362\u540e \\(i_0\\) \u548c \\(i\\) \u90fd\u52a0\u4e00\uff0c\\([i_0,i-1]\\) \u90fd\u662f 0 \u8fd9\u4e00\u6027\u8d28\u4ecd\u7136\u6210\u7acb\u3002</p> C++<pre><code>class Solution {\npublic:\n    void moveZeroes(vector&lt;int&gt;&amp; nums) {\n        int i0 = 0;\n        for(int i = 0; i &lt; nums.size(); i ++)\n        {\n            if(nums[i] != 0)\n            {    \n                swap(nums[i], nums[i0]);\n                i0 ++;\n            }\n        }\n    }\n};\n</code></pre>"},{"location":"ALGORITHM/Reflections/array/#_9","title":"\u6709\u5e8f\u6570\u7ec4\u7684\u5e73\u65b9","text":"<p>Note</p> <ol> <li>\u521d\u59cb\u5316\u957f\u4e3a \\(n\\) \u7684\u6570\u7ec4 \\(ans\\)\u3002</li> <li>\u5de6\u6307\u9488\\(i=0\\)\uff0c\u53f3\u6307\u9488\\(j=n-1\\)\u3002\u521d\u59cb\u5316\u4e0b\u6807\\(p=n-1\\)\uff0c\u8868\u793a\u5411\\(ans[p]\\)\u586b\u5165\u6570\u636e</li> <li>\u8bbe\\(x=nums[i]^2,y=nums[j]^2\\)</li> <li>\u82e5\\(x&gt;y\\)\uff0c\u5c06\\(x\\)\u586b\u5165\\(ans[p]\\)\uff0c\u53cd\u4e4b\u5c06\\(y\\)\u586b\u5165\\(ans[p]\\)</li> </ol> C++<pre><code>class Solution {\npublic:\n    vector&lt;int&gt; sortedSquares(vector&lt;int&gt;&amp; nums) {\n        int n = nums.size();\n        vector&lt;int&gt; ans(n);\n        int i = 0, j = n - 1;\n        for(int p = n - 1; p &gt;= 0; p --)\n        {\n            // int x = nums[i] * nums[i];\n            // int y = nums[j] * nums[j];\n            int x = nums[i], y = nums[j];\n            if(-x &gt; y)\n            {\n                ans[p] = x * x;\n                i ++;\n            }\n            else\n            {\n                ans[p] = y * y;\n                j --;\n            }\n        }\n        return ans;\n    }\n};\n</code></pre>"},{"location":"ALGORITHM/basic/1/","title":"\u6ed1\u52a8\u7a97\u53e3","text":""},{"location":"ALGORITHM/basic/1/#_2","title":"\u5b9a\u957f\u6ed1\u52a8\u7a97\u53e3","text":"<p>\u5957\u8def</p> <ol> <li>\u5165\uff1a\u4e0b\u6807\u4e3a \\(i\\) \u7684\u5143\u7d20\u8fdb\u5165\u7a97\u53e3\uff0c\u66f4\u65b0\u76f8\u5173\u7edf\u8ba1\u91cf\u3002\u5982\u679c \\(i&lt;k-1\\) \u5219\u91cd\u590d\u7b2c\u4e00\u6b65\u3002</li> <li>\u66f4\u65b0\uff1a\u66f4\u65b0\u7b54\u6848\u3002\u4e00\u822c\u662f\u6700\u5927\u503c/\u6700\u5c0f\u503c\u3002</li> <li>\u51fa\uff1a\u4e0b\u6807\u4e3a \\(i-k+1\\) \u7684\u5143\u7d20\u79bb\u5f00\u7a97\u53e3\uff0c\u66f4\u65b0\u76f8\u5173\u7edf\u8ba1\u91cf\u3002</li> </ol>"},{"location":"ALGORITHM/basic/1/#k","title":"\u534a\u5f84\u4e3a k \u7684\u5b50\u6570\u7ec4\u5e73\u5747\u503c","text":"<p>Note</p> <p>\u534a\u5f84\u4e3a \\(k\\)\uff0c\u7a97\u53e3\u957f\u5ea6\u5373\u4e3a \\(2*k+1\\)</p> C++<pre><code>class Solution {\npublic:\n    vector&lt;int&gt; getAverages(vector&lt;int&gt;&amp; nums, int k) {\n        long long sum = 0, len = 2 * k + 1;\n        vector&lt;int&gt; ret(nums.size(), -1);\n        for (int i = 0; i &lt; nums.size(); i++) {\n            //\u8fdb\u5165\u7a97\u53e3\n            sum += nums[i];\n            if (i &lt; 2 * k)\n                continue;\n            //\u66f4\u65b0\u7b54\u6848\n            ret[i - k] = sum / len;\n            //\u79bb\u5f00\u7a97\u53e3\n            sum -= nums[i - len + 1];\n        }\n        return ret;\n    }\n};\n</code></pre>"},{"location":"ALGORITHM/basic/1/#k_1","title":"\u5f97\u5230 K \u4e2a\u9ed1\u5757\u7684\u6700\u5c11\u6d82\u8272\u6b21\u6570","text":"C++<pre><code>class Solution {\npublic:\n    int minimumRecolors(string blocks, int k) {\n        int ans = 1e6, sum = 0;\n        for (int i = 0; i &lt; blocks.length(); i++) {\n            if (blocks[i] == 'W')\n                sum++;\n            if (i &lt; k - 1)\n                continue;\n            ans = min(ans, sum);\n            if (blocks[i - k + 1] == 'W')\n                sum--;\n        }\n        return ans;\n    }\n};\n</code></pre>"},{"location":"ALGORITHM/basic/1/#_3","title":"\u7231\u751f\u6c14\u7684\u4e66\u5e97\u8001\u677f","text":"<p>\u601d\u8def</p> <ol> <li>\\(S_0\\) \u8868\u793a\u8001\u677f\u4e0d\u751f\u6c14\u65f6\u987e\u5ba2\u7684\u6570\u91cf\u3002</li> <li>\\(max_{s1}\\) \u8868\u793a\u8001\u677f\u5728\u957f\u5ea6\u4e3a \\(minties\\) \u7684\u5b50\u6570\u7ec4\u4e2d\uff0c\u8001\u677f\u751f\u6c14\u65f6\u987e\u5ba2\u6570\u91cf\u7684\u6700\u5927\u503c\u3002\u6700\u7ec8\u7b54\u6848\u4e3a \\(s_0 + max_{s1}\\)</li> </ol> C++<pre><code>class Solution {\npublic:\n    int maxSatisfied(vector&lt;int&gt;&amp; customers, vector&lt;int&gt;&amp; grumpy, int minutes) {\n        int s[2]{}, max_s1 = 0;\n        for (int i = 0; i &lt; customers.size(); i++) {\n            s[grumpy[i]] += customers[i];\n            if (i &lt; minutes - 1)\n                continue;\n            max_s1 = max(s[1], max_s1);\n            if (grumpy[i - minutes + 1])\n                s[1] -= customers[i - minutes + 1];\n            else\n                s[1] -= 0;\n        }\n        return s[0] + max_s1;\n    }\n};\n</code></pre>"},{"location":"ALGORITHM/basic/1/#_4","title":"\u51e0\u4e4e\u552f\u4e00\u5b50\u6570\u7ec4\u7684\u6700\u5927\u548c","text":"<p>Note</p> <p>\u901a\u8fc7<code>cnt.size()</code>\u5224\u65ad\u5f53\u524d\u7a97\u53e3\u7684\u5143\u7d20\u662f\u5426\u81f3\u5c11\u4e3a\\(m\\)\u3002</p> C++<pre><code>class Solution {\npublic:\n    long long maxSum(vector&lt;int&gt;&amp; nums, int m, int k) {\n        long long ans = 0, sum = 0;\n        unordered_map&lt;int, int&gt; cnt;\n\n        for(int i = 0; i &lt; k - 1; i ++)\n        {\n            sum += nums[i];\n            cnt[nums[i]] ++;\n        }\n\n        for(int i = k - 1; i &lt; nums.size(); i ++)\n        {\n            sum += nums[i];\n            cnt[nums[i]] ++;\n            if(cnt.size() &gt;= m)\n                ans = max(ans, sum);\n            int out = nums[i - k + 1];\n            sum -= out;\n            if(-- cnt[out] == 0)\n                cnt.erase(out);            \n        }\n        return ans;\n\n    }\n};\n</code></pre>"},{"location":"ALGORITHM/basic/1/#_5","title":"\u53ef\u83b7\u5f97\u7684\u6700\u5927\u70b9\u6570","text":"C++<pre><code>class Solution {\npublic:\n    int maxScore(vector&lt;int&gt;&amp; cardPoints, int k) {\n        int n = cardPoints.size(), m = n - k;\n        int s = accumulate(cardPoints.begin(),cardPoints.begin() + m, 0);\n        int min_s = s;\n        for(int i = m; i &lt; n; i ++)\n        {\n            s += cardPoints[i] - cardPoints[i - m];\n            min_s = min(min_s, s);\n        }\n        return accumulate(cardPoints.begin(), cardPoints.end(), 0) - min_s;\n    }\n};\n</code></pre>"},{"location":"ALGORITHM/basic/1/#_6","title":"\u4e0d\u5b9a\u957f\u6ed1\u52a8\u7a97\u53e3","text":"<p>\u4e0d\u5b9a\u957f\u6ed1\u52a8\u7a97\u53e3\u4e3b\u8981\u5206\u4e09\u7c7b\uff1a\u6c42\u6700\u957f\u5b50\u6570\u7ec4\uff0c\u6c42\u6700\u77ed\u5b50\u6570\u7ec4\uff0c\u4ee5\u53ca\u6c42\u5b50\u6570\u7ec4\u4e2a\u6570\u3002</p>"},{"location":"ALGORITHM/basic/1/#_7","title":"\u6c42\u6700\u957f/\u6700\u5927","text":""},{"location":"ALGORITHM/basic/1/#_8","title":"\u65e0\u91cd\u590d\u5b57\u7b26\u7684\u6700\u957f\u5b50\u4e32","text":"C++<pre><code>class Solution {\npublic:\n    int lengthOfLongestSubstring(string s) {\n        int ans = 0, l = 0;\n        unordered_map&lt;char, int&gt; cnt;\n        for(int r = 0; r &lt; s.length(); r ++)\n        {\n            cnt[s[r]] ++;\n            while(cnt[s[r]] &gt; 1)\n            {\n                cnt[s[l]] --;\n                l ++;\n            }\n            ans = max(ans, r - l + 1);\n        }\n        return ans;\n    }\n};\n</code></pre>"},{"location":"ALGORITHM/basic/1/#_9","title":"\u5220\u9664\u5b50\u6570\u7ec4\u7684\u6700\u5927\u5f97\u5206","text":"C++<pre><code>class Solution {\npublic:\n    int maximumUniqueSubarray(vector&lt;int&gt;&amp; nums) {\n        int ans = 0, sum = 0, l = 0;\n        unordered_map&lt;int, int&gt; cnt;\n        for(int r = 0; r &lt; nums.size(); r ++)\n        {\n            sum += nums[r];\n            cnt[nums[r]] ++;\n            while(cnt[nums[r]] &gt; 1)\n            {\n                cnt[nums[l]] --;\n                sum -= nums[l++];\n            }\n            ans = max(ans, sum);\n        }\n        return ans;\n    }\n};\n</code></pre>"},{"location":"ALGORITHM/basic/1/#_10","title":"\u6c42\u6700\u77ed/\u6700\u5c0f","text":""},{"location":"ALGORITHM/basic/1/#_11","title":"\u957f\u5ea6\u6700\u5c0f\u7684\u5b50\u6570\u7ec4","text":"C++<pre><code>class Solution {\npublic:\n    int minSubArrayLen(int target, vector&lt;int&gt;&amp; nums)\n    {\n        // int n = nums.size(), ans = n + 1, sum = 0, l = 0;\n        // for(int r = 0; r &lt; n; r ++)\n        // {\n        //     sum += nums[r];\n        //     while(sum - nums[l] &gt;= target)\n        //         sum -= nums[l++];\n        //     if(sum &gt;= target)\n        //         ans = min(ans, r - l + 1);\n        // }\n        // return ans &lt;= n ? ans: 0;\n        int ans = n + 1, sum = 0, l = 0;\n        for(int r = 0; r &lt; nums.size();  r++)\n        {\n            sum += nums[r];\n            while(sum &gt;= target)\n            {\n                ans = min(ans, l - r + 1);\n                sum -= nums[l ++];\n            }\n        }\n        return ans &lt;= n ? ans : 0;\n    }\n};\n</code></pre>"},{"location":"ALGORITHM/basic/2/","title":"\u53cc\u6307\u9488","text":""},{"location":"ALGORITHM/basic/2/#1","title":"\u76f8\u5411\u53cc\u6307\u9488 1","text":"<p>Note</p> <p>\u4e24\u4e2a\u6307\u9488 \\(l=0,r=n-1\\)\uff0c\u4ece\u6570\u7ec4\u7684\u4e24\u7aef\u5f00\u59cb\u5411\u4e2d\u95f4\u79fb\u52a8\uff0c\u79f0\u4e3a\u53cc\u5411\u53cc\u6307\u9488\u3002</p>"},{"location":"ALGORITHM/basic/2/#ii-","title":"\u4e24\u6570\u4e4b\u548c II - \u8f93\u5165\u6709\u5e8f\u6570\u7ec4","text":"<p>\u601d\u8def</p> <ol> <li>\u56e0\u4e3a\u6570\u7ec4\u4e3a\u6709\u5e8f\uff0c\u6545\u4ece\u6570\u7ec4\u4e24\u7aef\u5f00\u59cb\uff0c\u5c06\u6700\u5927\u503c\u4e0e\u6700\u5c0f\u503c\u76f8\u52a0\u3002</li> <li>\u82e5\u7ed3\u679c\u5927\u4e8e\u76ee\u6807\u503c\uff0c\u5c06\u6700\u5927\u503c\u4ece\u6570\u7ec4\u5220\u9664\uff1b\u82e5\u7ed3\u679c\u5c0f\u4e8e\u76ee\u6807\u503c\uff0c\u5c06\u6700\u5c0f\u503c\u4ece\u6570\u7ec4\u5220\u9664\u3002</li> <li>\u9898\u76ee\u4fdd\u8bc1\u6709\u89e3\uff0c\u8fd4\u56de\u7ed3\u679c\u3002</li> </ol> C++<pre><code>class Solution {\npublic:\n    vector&lt;int&gt; twoSum(vector&lt;int&gt;&amp; n, int x) {\n        int l = 0, r = n.size() - 1;\n        while(true)\n        {\n            int s = n[l] + n[r];\n            if(s == x)\n                return {l + 1, r + 1};\n            else if(s &gt; x)\n                r --;\n            else\n                l ++;\n        }\n    }\n};\n</code></pre>"},{"location":"ALGORITHM/basic/2/#_2","title":"\u4e09\u6570\u4e4b\u548c","text":"<p>\u672c\u8d28\u8fd8\u662f\u4e24\u6570\u4e4b\u548c\uff0c\u679a\u4e3e\u5230 \\(nums[i]\\) \u65f6\uff0c\u53ea\u9700\u5224\u65ad\u5269\u4f59\u4e24\u6570\u4e4b\u548c\u662f\u5426\u7b49\u4e8e \\(-nums[i]\\) \u5373\u53ef\u3002</p> <p>\u4f18\u5316</p> <ul> <li>\u5f53 \\(x + nums[i + 1] + nums[i + 2] &gt; 0\\) \u65f6\uff0c\u8868\u660e\u6700\u5c0f\u7684\u4e09\u4e2a\u6570\u76f8\u52a0\u5927\u4e8e \\(0\\)\uff0c\u8fd4\u56de\u7a7a vector;</li> <li>\u5f53 \\(x + nums[n - 2] + nums[n - 1] &lt; 0\\) \u65f6\uff0c\u8868\u660e\u5f53\u524d\u6ca1\u6709\u6570\u4e0e \\(x\\) \u76f8\u52a0\u5f97\u96f6\uff0ccontinue \u904d\u5386\u4e0b\u4e00\u4e2a \\(x\\)\u3002</li> </ul> C++<pre><code>class Solution {\npublic:\n    vector&lt;vector&lt;int&gt;&gt; threeSum(vector&lt;int&gt;&amp; nums) {\n        sort(nums.begin(), nums.end());\n        vector&lt;vector&lt;int&gt;&gt; ans;\n        int n = nums.size();\n        for(int i = 0; i &lt; n - 2; i ++)\n        {\n            int x = nums[i];\n            if(i &gt; 0 &amp;&amp; x == nums[i - 1])\n                continue;\n            if(x + nums[i + 1] + nums[i + 2] &gt; 0)\n                break;\n            if(x + nums[n - 2]+ nums[n - 1] &lt; 0)\n                continue;\n            int j = i + 1, k = n - 1;\n            while(j &lt; k)\n            {\n                int s = x + nums[j] + nums[k];\n                if(s &gt; 0)\n                    k --;\n                else if(s &lt; 0)\n                    j ++;\n                else\n                {\n                    ans.push_back({x, nums[j], nums[k]});\n                    for(j ++; j &lt; k &amp;&amp; nums[j] == nums[j - 1]; j ++);\n                    for(k --; k &gt; j &amp;&amp; nums[k] == nums[k + 1]; k --);\n                }\n            }\n        }\n        return ans;\n    }\n};\n</code></pre>"},{"location":"ALGORITHM/basic/2/#_3","title":"\u7edf\u8ba1\u548c\u5c0f\u4e8e\u76ee\u6807\u7684\u4e0b\u6807\u5bf9\u6570\u76ee","text":"<p>\u601d\u8def</p> <ol> <li>\u5c06\u6700\u5927\u503c\u4e0e\u6700\u5c0f\u503c\u76f8\u52a0\u3002</li> <li>\u82e5\u7ed3\u679c\u5c0f\u4e8e\u76ee\u6807\u503c\uff0c\u5219 \\([l,r]\\) \u4e4b\u95f4\u7684\u6bcf\u4e2a\u6570\u4e0e \\(l\\) \u90fd\u6784\u6210\u7b54\u6848\u3002</li> <li>\u5426\u5219 \\(r--\\)</li> </ol> C++<pre><code>class Solution {\npublic:\n    int countPairs(vector&lt;int&gt;&amp; nums, int target) {\n        sort(nums.begin(), nums.end());\n        int l = 0, r = nums.size() - 1;\n        int res = 0;\n        while(l &lt; r)\n        {\n            int x = nums[l] + nums[r];\n            if(x &lt; target)\n            {\n                res += r - l;\n                l ++;\n            }\n            else\n                r --;\n        }\n        return res;\n    }\n};\n</code></pre>"},{"location":"ALGORITHM/basic/2/#2","title":"\u76f8\u5411\u53cc\u6307\u9488 2","text":""},{"location":"ALGORITHM/basic/2/#_4","title":"\u76db\u6700\u591a\u6c34\u7684\u5bb9\u5668","text":"<p>\u601d\u8def</p> <p>\u76db\u6c34\u7684\u591a\u5c11\u662f\u7531\u6700\u77ed\u8fb9\u51b3\u5b9a\u7684\uff0c\u6240\u4ee5\u6bcf\u6b21\u90fd\u5e94\u8be5\u66f4\u65b0\u6700\u77ed\u8fb9\uff0c\u7136\u540e\u66f4\u65b0\\(ans = max(ans, area)\\)\u3002\u5982\u679c\u66f4\u65b0\u6700\u957f\u8fb9\uff0c\u5219\u56e0\u4e3a\u77ed\u8fb9\u7684\u5b58\u5728\uff0c\u5e95\u8fb9\u53d8\u5c0f\uff0c\u9762\u79ef\u4e00\u5b9a\u4e0d\u4f1a\u53d8\u5927\u3002</p> C++<pre><code>class Solution {\npublic:\n    int maxArea(vector&lt;int&gt;&amp; height) {\n        int ans = 0, l = 0, r = height.size() - 1;\n        while(l &lt; r)\n        {\n            int area = (r - l) * min(height[l], height[r]);\n            ans = max(ans, area);\n            if(height[l] &lt; height[r])\n                l ++;\n            else\n                r --;\n        }\n        return ans;\n    }\n};\n</code></pre>"},{"location":"ALGORITHM/basic/search/","title":"\u641c\u7d22","text":""},{"location":"ALGORITHM/basic/search/#_2","title":"\u56fe\u7684\u5b58\u50a8","text":"<p>\u56fe\u7684\u5b58\u50a8\u65b9\u5f0f\u4ee5\u53ca dfs \u793a\u4f8b\u3002</p>"},{"location":"ALGORITHM/basic/search/#_3","title":"\u90bb\u63a5\u77e9\u9635","text":"C++<pre><code>#include &lt;iostream&gt;\n#include &lt;cstring&gt;\n#include &lt;algorithm&gt;\n\nusing namespace std;\n\nconst int N = 1010;\nint n, m, a, b, c;\nint w[N][N];\nbool vis[N];\n\nvoid dfs(int u)\n{\n    vis[u] = true;\n    for (int v = 1; v &lt;= n; v++)\n    {\n        if (w[u][v])\n        {\n            cout &lt;&lt; u &lt;&lt; v &lt;&lt; w[u][v] &lt;&lt; endl;\n            if (!vis[v])\n                dfs(v);\n        }\n    }\n}\n\nint main()\n{\n    cin &gt;&gt; n &gt;&gt; m;\n    while (m--)\n    {\n        cin &gt;&gt; a &gt;&gt; b &gt;&gt; c;\n        w[a][b] = c;\n    }\n    dfs(1)\n    return 0;\n}\n</code></pre>"},{"location":"ALGORITHM/basic/search/#_4","title":"\u90bb\u63a5\u8868","text":"C++<pre><code>//\u65e0\u8fb9\u6743\n#include &lt;iostream&gt;\n#include &lt;cstring&gt;\n#include &lt;algorithm&gt;\n#include&lt;vector&gt;\n\nusing namespace std;\n\nconst int N = 1010;\nint n, m, a, b;\nvector&lt;int&gt; e[N];\nbool vis[N];\n\nvoid dfs(int u)\n{\n    vis[u] = true;\n    for (auto ed : e[u])\n    {\n        int v = ed;\n        cout &lt;&lt; u &lt;&lt; v;\n        if (!vis[v])\n            dfs(v);\n    }\n}\n\nint main()\n{\n    cin &gt;&gt; n &gt;&gt; m;\n    while(m --)\n    {\n        cin &gt;&gt; a &gt;&gt; b;\n        e[a].push_back(b);\n    }\n    return 0;\n}\n\n//\u6709\u8fb9\u6743\nstruct edge\n{\n    int v, w; //\u8fb9\u7684\u7ec8\u70b9v\u548c\u6743\u91cdw\n}\nvector&lt;edge&gt; e[N];\n\nint main()\n{\n    cin &gt;&gt; n &gt;&gt; m;\n    while(m --)\n    {\n        cin &gt;&gt; a &gt;&gt; b &gt;&gt; c;\n        e[a].push_back({b, c});\n    }\n    return 0;\n}\n</code></pre>"},{"location":"ALGORITHM/basic/search/#_5","title":"\u8fb9\u96c6\u6570\u7ec4","text":"C++<pre><code>#include &lt;iostream&gt;\n#include &lt;cstring&gt;\n#include &lt;algorithm&gt;\nusing namespace std;\n\nconst int N = 1010, M = 1010;\nint n, m, a, b, c;\nstruct edge\n{\n    int u, v, w;\n} e[M];\nbool vis[N];\n\nvoid dfs(int u)\n{\n    vis[u] = true;\n    for (int i = 1; i &lt;= m; i++)\n    {\n        if (e[i].u == u)\n        {\n            int v = e[i].v, w = e[i].w;\n            cout &lt;&lt; u &lt;&lt; v &lt;&lt; w;\n            if (!vis[v])\n                dfs(v);\n        }\n    }\n}\n\nint main()\n{\n    cin &gt;&gt; n &gt;&gt; m;\n    for (int i = 1; i &lt;= m; i++)\n    {\n        cin &gt;&gt; a &gt;&gt; b &gt;&gt; c;\n        e[i] = {a, b, c};\n    }\n    return 0;\n}\n</code></pre>"},{"location":"ALGORITHM/basic/search/#_6","title":"\u94fe\u5f0f\u90bb\u63a5\u8868","text":"C++<pre><code>struct edge \n{\n    int v, w;\n};\nvector&lt;edge&gt; e;\nvector&lt;int&gt; h[N];\n</code></pre> <ul> <li>struct edge \u4e3a\u8fb9\u7684\u7ed3\u6784\u4f53\u7c7b\u578b\uff0cv \u4e3a\u7ec8\u70b9\uff0cw \u4e3a\u6743\u91cd\u3002</li> <li>vector e \u662f\u4e00\u4e2a\u8fb9\u7684\u96c6\u5408\uff0c\u7528\u4e8e\u5b58\u50a8\u56fe\u4e2d\u7684\u6240\u6709\u8fb9\u3002</li> <li>h [N]\uff1a\u90bb\u63a5\u8868\uff0ch [i] \u5b58\u50a8\u7ed3\u70b9 i \u76f8\u5173\u7684\u6240\u6709\u51fa\u8fb9\u518d e \u4e2d\u7684\u7d22\u5f15\u3002</li> </ul> C++<pre><code>void add(int a, int b, int c) {\n    e.push_back({b, c});\n    h[a].push_back(e.size() - 1);\n}\n</code></pre> <ul> <li><code>e.push_back({b, c});</code>\uff1a\u5728\u8fb9\u96c6 e \u4e2d\u6dfb\u52a0\u4e00\u6761\u65b0\u7684\u8fb9\uff0c\u8d77\u70b9\u4e3a a\uff0c\u7ec8\u70b9\u4e3a b\uff0c\u6743\u91cd\u4e3a c\u3002</li> <li><code>h[a].push_back(e.size() - 1);</code>\uff1a\u8bb0\u5f55\u8fd9\u6761\u8fb9\u5728 e \u4e2d\u7684\u7d22\u5f15\uff0c\u5e76\u50a8\u5b58\u5728 h [a] \u4e2d\uff0c\u8868\u793a\u7ed3\u70b9 a \u7684\u4e00\u6761\u50a8\u8fb9\u3002<code>e.size() - 1</code> \u662f\u8fb9\u5728 e \u4e2d\u7684\u7d22\u5f15</li> </ul> C++<pre><code>void dfs(int u)\n{\n    vis[u] = true;\n    for (int i = 0; i &lt; h[u].size(); i++)\n    {\n        int j = h[u][i];\n        int v = e[j].v, w = e[j].w;\n        cout &lt;&lt; u &lt;&lt; v &lt;&lt; w;\n        if (!vis[v])\n            dfs(v);\n    }\n}\n\nint main() {\n    cin &gt;&gt; n &gt;&gt; m;\n    for (int i = 1; i &lt;= m; i++) {\n        cin &gt;&gt; a &gt;&gt; b &gt;&gt; c;\n        add(a, b, c); // \u6dfb\u52a0\u8fb9 a-&gt;b\n        add(b, a, c); // \u6dfb\u52a0\u8fb9 b-&gt;a (\u56e0\u4e3a\u56fe\u662f\u65e0\u5411\u7684)\n    }\n    dfs(1);  \n    return 0;\n}\n</code></pre> <p>\u5047\u8bbe\u56fe\u4e2d\u6709\u4ee5\u4e0b\u8fb9\uff1a</p> <ul> <li>(1, 2, 5)\uff1a\u4ece\u8282\u70b9 1 \u5230\u8282\u70b9 2 \u7684\u6743\u91cd\u4e3a 5 \u7684\u8fb9\u3002</li> <li>(2, 3, 3)\uff1a\u4ece\u8282\u70b9 2 \u5230\u8282\u70b9 3 \u7684\u6743\u91cd\u4e3a 3 \u7684\u8fb9\u3002</li> </ul> <p>\u5f53\u6211\u4eec\u8c03\u7528 add \u51fd\u6570\u540e\uff0ce \u548c h \u7684\u72b6\u6001\u5982\u4e0b\uff1a</p> <ul> <li>e (\u8fb9\u96c6)\uff1a</li> <li>e [0]\uff1a{2, 5}\uff08\u4ee3\u8868\u8fb9 1-&gt; 2\uff0c\u6743\u91cd 5\uff09 </li> <li>e [1]\uff1a{1, 5}\uff08\u4ee3\u8868\u8fb9 2-&gt; 1\uff0c\u6743\u91cd 5\uff0c\u8868\u793a\u65e0\u5411\u56fe\u7684\u53e6\u4e00\u65b9\u5411\uff09</li> <li>e [2]\uff1a{3, 3}\uff08\u4ee3\u8868\u8fb9 2-&gt; 3\uff0c\u6743\u91cd 3\uff09</li> <li>e [3]\uff1a{2, 3}\uff08\u4ee3\u8868\u8fb9 3-&gt; 2\uff0c\u6743\u91cd 3\uff09</li> <li>h (\u90bb\u63a5\u8868)\uff1a</li> <li>h [1]\uff1a[0]\uff08h [1] \u8868\u793a\u8282\u70b9 1 \u7684\u51fa\u8fb9\u96c6\u5408\uff0c\u51fa\u8fb9\u4e3a e [0]\uff09</li> <li>h [2]\uff1a[1, 2]\uff08h [2] \u8868\u793a\u8282\u70b9 2 \u7684\u51fa\u8fb9\u96c6\u5408\uff0c\u51fa\u8fb9\u4e3a e [1] \u548c e [2]\uff09</li> <li>h [3]\uff1a[3]\uff08h [3] \u8868\u793a\u8282\u70b9 3 \u7684\u51fa\u8fb9\u96c6\u5408\uff0c\u51fa\u8fb9\u4e3a e [3]\uff09</li> </ul>"},{"location":"ALGORITHM/basic/search/#_7","title":"\u94fe\u5f0f\u524d\u5411\u661f","text":"C++<pre><code>struct edge\n{\n    int v, w, ne;\n} e[M];\nint idx, h[N];\n</code></pre> <ul> <li>struct edge \u5b9a\u4e49\u4e86\u8fb9\u7684\u7ed3\u6784\u4f53\uff0cv \u4e3a\u8fb9\u7684\u7ec8\u70b9\uff0cw \u4e3a\u6743\u91cd\uff0cne \u4e3a\u4e0b\u6761\u8fb9\u7684\u7d22\u5f15</li> <li>idx \u4e3a\u5f53\u524d\u8fb9\u7684\u7d22\u5f15\uff0c\u6bcf\u6dfb\u52a0\u4e00\u6761\u8fb9\u540e\u9012\u589e</li> <li>h [N] \u5b58\u50a8\u6bcf\u4e2a\u7ed3\u70b9\u7684\u7b2c\u4e00\u6761\u51fa\u8fb9\u7d22\u5f15</li> </ul> C++<pre><code>void add(int a, int b, int c) {\n    e[idx] = {b, c, h[a]}; // \u5c06\u8fb9 (a -&gt; b, \u6743\u91cd\u4e3a c) \u6dfb\u52a0\u5230\u8fb9\u96c6\u4e2d\n    h[a] = idx++; // \u66f4\u65b0\u8282\u70b9 a \u7684\u7b2c\u4e00\u6761\u51fa\u8fb9\n}\n\nvoid dfs(int u)\n{\n    vis[u] = true;\n    for(int i = h[u]; ~i; i = e[i].ne)\n    {\n        int v = e[i].v, w = e[i].w;\n        cout &lt;&lt; u &lt;&lt; v &lt;&lt; w;\n        if(!vis[v])\n            dfs(v);\n    }\n}\n\nint main() {\n    cin &gt;&gt; n &gt;&gt; m; // \u8f93\u5165\u8282\u70b9\u6570\u548c\u8fb9\u6570\n    memset(h, -1, sizeof h); // \u521d\u59cb\u5316\u90bb\u63a5\u8868\n    for (int i = 1; i &lt;= m; i++) {\n        cin &gt;&gt; a &gt;&gt; b &gt;&gt; c; // \u8f93\u5165\u8fb9\u7684\u7aef\u70b9\u548c\u6743\u91cd\n        add(a, b, c); // \u6dfb\u52a0\u8fb9\n        add(b, a, c); // \u5982\u679c\u662f\u65e0\u5411\u56fe\uff0c\u5219\u6dfb\u52a0\u53cd\u5411\u8fb9\n    }\n    dfs(1)\n    return 0;\n}\n</code></pre>"},{"location":"DEV/","title":"\u5f00\u53d1","text":"<p>\u70b9\u51fb\u5de6\u4fa7\u6253\u5f00\u76ee\u5f55</p>"},{"location":"DeepL/","title":"\u6df1\u5ea6\u5b66\u4e60","text":"<p>\u70b9\u51fb\u5de6\u4fa7\u6253\u5f00\u76ee\u5f55</p>"},{"location":"DeepL/Paper/Autoformer/","title":"Autoformer","text":"<p>\u5728\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u4e2d\uff0c\u9010\u70b9\u8fde\u63a5\uff08point-wise connection\uff09 \u901a\u5e38\u6307\u7684\u662f\u6a21\u578b\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\u5bf9\u5f53\u524d\u65f6\u523b\u7684\u7279\u5f81\u8fdb\u884c\u5904\u7406\u65f6\uff0c\u53ea\u8003\u8651\u8be5\u65f6\u523b\u4e0e\u5176\u4ed6\u65f6\u523b\u4e4b\u95f4\u7684\u5c40\u90e8\u6216\u76f4\u63a5\u8054\u7cfb\uff0c\u800c\u4e0d\u8003\u8651\u5168\u5c40\u6216\u957f\u671f\u4f9d\u8d56\u5173\u7cfb\u3002\u8fd9\u79cd\u8fde\u63a5\u65b9\u5f0f\u5e38\u89c1\u4e8e\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u7684 \u9010\u70b9\u8868\u793a\u805a\u5408\uff0c\u4e5f\u5c31\u662f\u8bf4\uff0c\u6a21\u578b\u5728\u8fdb\u884c\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u65f6\uff0c\u5bf9\u4e8e\u6bcf\u4e00\u4e2a\u65f6\u95f4\u6b65\uff0c\u5b83\u4ec5\u4ec5\u5229\u7528\u8be5\u65f6\u95f4\u6b65\u7684\u7279\u5f81\u4e0e\u5176\u4ed6\u5c11\u91cf\u7684\u65f6\u95f4\u6b65\u7684\u7279\u5f81\u8fdb\u884c\u76f8\u4e92\u4f5c\u7528\u3002</p> <p>\u9010\u70b9\u8fde\u63a5\u7684\u7279\u70b9\uff1a</p> <ul> <li> <p>\u5c40\u90e8\u6027\uff1a\u9010\u70b9\u8fde\u63a5\u901a\u5e38\u4f9d\u8d56\u4e8e\u65f6\u95f4\u6b65\u4e4b\u95f4\u7684\u5c40\u90e8\u76f8\u4e92\u5173\u7cfb\u3002\u4f8b\u5982\uff0c\u5728\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4e2d\uff0c\u9010\u70b9\u8fde\u63a5\u53ef\u80fd\u4ec5\u805a\u7126\u4e8e\u5f53\u524d\u65f6\u95f4\u6b65\u4e0e\u524d\u51e0\u4e2a\u65f6\u95f4\u6b65\u3001\u540e\u51e0\u4e2a\u65f6\u95f4\u6b65\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5ffd\u7565\u4e86\u957f\u65f6\u95f4\u8de8\u5ea6\u7684\u4f9d\u8d56\u3002</p> </li> <li> <p>\u7a00\u758f\u6027\uff1a\u968f\u7740\u5e8f\u5217\u957f\u5ea6\u7684\u589e\u52a0\uff0c\u9010\u70b9\u8fde\u63a5\u901a\u5e38\u4f1a\u91c7\u53d6\u7a00\u758f\u7b56\u7565\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\u8fd9\u79cd\u7a00\u758f\u8fde\u63a5\u5e38\u5e38\u610f\u5473\u7740\u53ea\u5173\u6ce8\u67d0\u4e9b\u5173\u952e\u70b9\u6216\u8ddd\u79bb\u8f83\u8fd1\u7684\u65f6\u95f4\u6b65\u4e4b\u95f4\u7684\u5173\u7cfb\u3002</p> </li> <li> <p>\u4fe1\u606f\u5c40\u9650\u6027\uff1a\u5c3d\u7ba1\u9010\u70b9\u8fde\u63a5\u5728\u8ba1\u7b97\u4e0a\u6bd4\u8f83\u9ad8\u6548\uff0c\u4f46\u5b83\u5728\u957f\u671f\u4f9d\u8d56\u5173\u7cfb\u7684\u6355\u6349\u4e0a\u53ef\u80fd\u5b58\u5728\u5c40\u9650\u6027\u3002\u5c24\u5176\u662f\u5728\u6d89\u53ca\u5230\u5468\u671f\u6027\u6a21\u5f0f\u6216\u590d\u6742\u65f6\u95f4\u6a21\u5f0f\u7684\u4efb\u52a1\u4e2d\uff0c\u9010\u70b9\u8fde\u63a5\u53ef\u80fd\u65e0\u6cd5\u5145\u5206\u6355\u6349\u5230\u8de8\u8d8a\u957f\u65f6\u95f4\u6bb5\u7684\u6df1\u5c42\u6b21\u4f9d\u8d56\u5173\u7cfb\u3002</p> </li> </ul> <p>\u5728\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\uff0c\u9010\u70b9\u8fde\u63a5\u7684\u5c40\u9650\u6027\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u8fdb\u884c\u6539\u8fdb\uff1a</p> <ul> <li> <p>\u5468\u671f\u6027\u4f9d\u8d56\uff1a\u901a\u8fc7\u8003\u8651\u65f6\u95f4\u5e8f\u5217\u7684\u5468\u671f\u6027\u7279\u5f81\uff0c\u5728\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u5f15\u5165\u66f4\u4e3a\u7075\u6d3b\u7684\u8fde\u63a5\u65b9\u5f0f\u3002\u6bd4\u5982\uff0c\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u5468\u671f\u6027\u7684\u7279\u5f81\u6765\u6784\u5efa\u65b0\u7684\u8fde\u63a5\u89c4\u5219\uff0c\u4e0d\u4ec5\u4ec5\u5173\u6ce8\u5c40\u90e8\u7684\u65f6\u95f4\u6b65\uff0c\u800c\u662f\u901a\u8fc7\u5468\u671f\u6027\u6355\u6349\u66f4\u5e7f\u6cdb\u7684\u65f6\u95f4\u4f9d\u8d56\u3002</p> </li> <li> <p>\u5e8f\u5217\u7ea7\u522b\u7684\u8fde\u63a5\uff1a\u53ef\u4ee5\u901a\u8fc7\u5206\u89e3\u7b56\u7565\uff0c\u5c06\u65f6\u95f4\u5e8f\u5217\u5206\u4e3a\u591a\u4e2a\u5b50\u5e8f\u5217\uff08\u4f8b\u5982\u5468\u671f\u6027\u5b50\u5e8f\u5217\uff09\uff0c\u5e76\u5728\u4e0d\u540c\u5b50\u5e8f\u5217\u4e4b\u95f4\u5efa\u7acb\u8fde\u63a5\uff0c\u8fd9\u6837\u6a21\u578b\u4e0d\u4ec5\u80fd\u591f\u6355\u6349\u5230\u5c40\u90e8\u7684\u4f9d\u8d56\uff0c\u8fd8\u80fd\u5728\u8f83\u957f\u65f6\u95f4\u8de8\u5ea6\u4e0a\u53d1\u73b0\u6f5c\u5728\u7684\u4f9d\u8d56\u5173\u7cfb\u3002</p> </li> <li> <p>\u5168\u5c40\u6ce8\u610f\u529b\u673a\u5236\uff1a\u4f8b\u5982\uff0c\u5168\u5c40\u81ea\u6ce8\u610f\u529b\uff08global self-attention\uff09\u6a21\u578b\u80fd\u591f\u540c\u65f6\u5173\u6ce8\u957f\u65f6\u95f4\u8de8\u5ea6\u5185\u7684\u6240\u6709\u65f6\u95f4\u6b65\uff0c\u4f7f\u5f97\u6a21\u578b\u5728\u8ba1\u7b97\u65f6\u80fd\u591f\u7efc\u5408\u8003\u8651\u957f\u8ddd\u79bb\u7684\u65f6\u95f4\u4f9d\u8d56\uff0c\u89e3\u51b3\u9010\u70b9\u8fde\u63a5\u65e0\u6cd5\u6355\u6349\u5230\u957f\u671f\u4f9d\u8d56\u7684\u95ee\u9898\u3002</p> </li> </ul>"},{"location":"DeepL/Paper/Autoformer/#_1","title":"\u89e3\u51b3\u95ee\u9898\uff1a\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b","text":"<p>\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u95ee\u9898\uff1a\u5f85\u9884\u6d4b\u7684\u5e8f\u5217\u957f\u5ea6\u8fdc\u5927\u4e8e\u8f93\u5165\u957f\u5ea6\uff0c\u5373\u57fa\u4e8e\u6709\u9650\u7684\u4fe1\u606f\u9884\u6d4b\u66f4\u957f\u8fdc\u7684\u672a\u6765\u3002</p> \u57fa\u4e8e transformer \u7684\u9884\u6d4b\u6a21\u578b Autoformer \u5e94\u5bf9\u590d\u6742\u65f6\u95f4\u6a21\u5f0f \u96be\u4ee5\u76f4\u63a5\u53d1\u73b0\u53ef\u9760\u7684\u65f6\u95f4\u4f9d\u8d56 \u6df1\u5ea6\u5206\u89e3\u67b6\u6784 \u957f\u5e8f\u5217\u9ad8\u6548\u5904\u7406 \u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\u5bfc\u81f4\u4fe1\u606f\u5229\u7528\u74f6\u9888 \u81ea\u76f8\u5173\u673a\u5236"},{"location":"DeepL/Paper/Autoformer/#autoformer_1","title":"Autoformer \u521b\u65b0\u70b9","text":"<ul> <li>\u7a81\u7834\u5c06\u5e8f\u5217\u5206\u89e3\u4f5c\u4e3a\u9884\u5904\u7406\u7684\u4f20\u7edf\u65b9\u6cd5\uff0c\u63d0\u51fa\u6df1\u5ea6\u5206\u89e3\u67b6\u6784\uff0c\u80fd\u591f\u4ece\u590d\u6742\u65f6\u95f4\u6a21\u5f0f\u4e2d\u5206\u89e3\u51fa\u53ef\u9884\u6d4b\u6027\u66f4\u5f3a\u7684\u7ec4\u6210\u90e8\u5206\u3002</li> <li>\u57fa\u4e8e\u968f\u673a\u8fc7\u7a0b\u7406\u8bba\uff0c\u63d0\u51fa\u81ea\u76f8\u5173\u673a\u5236\uff0c\u4ee3\u66ff\u9010\u70b9\u94fe\u63a5\u7684\u6ce8\u610f\u529b\u673a\u5236\u3002\u5b9e\u73b0\u5e8f\u5217\u7ea7\uff08series-wise\uff09\u8fde\u63a5\u548c \\(O(LlogL)\\) \u590d\u6742\u5ea6\u3002</li> </ul>"},{"location":"DeepL/Paper/Autoformer/#autoformer_2","title":"Autoformer \u67b6\u6784","text":""},{"location":"DeepL/Paper/Autoformer/#_2","title":"\u6df1\u5ea6\u5206\u89e3\u67b6\u6784","text":"<p>\u65f6\u95f4\u5e8f\u5217\u5206\u89e3\u65e8\u5728\u5c06\u4e00\u4e2a\u590d\u6742\u7684\u65f6\u95f4\u5e8f\u5217\u5206\u89e3\u4e3a\u591a\u4e2a\u76f8\u5bf9\u7b80\u5355\u7684\u7ec4\u6210\u90e8\u5206, \u4ee5\u4fbf\u66f4\u597d\u5730\u7406\u89e3\u5176\u7279\u5f81\u548c\u89c4\u5f8b\u3002\u901a\u5e38\u5305\u62ec\uff1a</p> <ul> <li>\u8d8b\u52bf\uff1a\u8868\u793a\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u957f\u671f\u7684\u4e0a\u5347\u6216\u4e0b\u964d\u8d8b\u52bf\u3002</li> <li>\u5b63\u8282\u6027\uff1a\u8868\u793a\u6570\u636e\u4e2d\u5468\u671f\u6027\u91cd\u590d\u7684\u6a21\u5f0f\uff0c\u901a\u5e38\u4e0e\u5b63\u8282\u3001\u6708\u4efd\u6216\u4e00\u5468\u4e2d\u7684\u7279\u5b9a\u65f6\u95f4\u6709\u5173\u3002</li> <li>\u6b8b\u5dee\uff1a\u8868\u793a\u65f6\u95f4\u5e8f\u5217\u4e2d\u65e0\u6cd5\u7531\u8d8b\u52bf\u548c\u5b63\u8282\u6027\u6a21\u5f0f\u89e3\u91ca\u7684\u968f\u673a\u6ce2\u52a8\u3002</li> </ul> <p>\u5e38\u7528\u5206\u89e3\u65b9\u6cd5\u4e3a\u52a0\u6cd5\u5206\u89e3\uff1a\\(Y_t = T_t + S_t + R_t\\)\u3002\u7531\u4e8e\u9884\u6d4b\u95ee\u9898\u4e2d\u672a\u6765\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u901a\u5e38\u5148\u5bf9\u8fc7\u53bb\u5e8f\u5217\u8fdb\u884c\u5206\u89e3\uff0c\u518d\u5206\u522b\u9884\u6d4b\u3002\u4f46\u8fd9\u4f1a\u9020\u6210\u9884\u6d4b\u7ed3\u679c\u53d7\u9650\u4e8e\u5206\u89e3\u6548\u679c\uff0c\u5e76\u5ffd\u89c6\u4e86\u672a\u6765\u5404\u4e2a\u7ec4\u6210\u90e8\u5206\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002</p> <p>\u672c\u6587\u63d0\u51fa\u6df1\u5ea6\u5206\u89e3\u67b6\u6784\uff0c\u5c06\u5e8f\u5217\u5206\u89e3\u4f5c\u4e3a\u6a21\u578b\u7684\u4e00\u4e2a\u5185\u90e8\u5355\u5143\uff0c\u5d4c\u5165\u5230\u7f16-\u89e3\u7801\u5668\u4e2d\u3002\u5728\u9884\u6d4b\u8fc7\u7a0b\u4e2d\uff0c\u6a21\u578b\u4ea4\u66ff\u8fdb\u884c\u9884\u6d4b\u7ed3\u679c\u7684\u4f18\u5316\u548c\u5e8f\u5217\u5206\u89e3\uff0c\u5373\u4ece\u9690\u53d8\u91cf\u4e2d\u9010\u6b65\u5206\u79bb\u8d8b\u52bf\u9879\u4e0e\u5468\u671f\u9879\uff0c\u5b9e\u73b0\u6e10\u8fdb\u5f0f\u5206\u89e3\u3002</p> <p>\u672c\u6587\u5c06\u5e8f\u5217\u5206\u89e3\u4e3a\u8d8b\u52bf\u9879\u548c\u5b63\u8282\u9879\uff0c\u91c7\u7528 AvgPool \u8fdb\u884c\u79fb\u52a8\u5e73\u5747\uff0c\u5e76\u8fdb\u884c\u586b\u5145\u64cd\u4f5c\u4ee5\u4fdd\u6301\u5e8f\u5217\u957f\u5ea6\u4e0d\u53d8\u3002\uff08AvgPool \u662f\u4e00\u79cd\u4e0b\u91c7\u6837\u65b9\u6cd5\uff0c\u7528\u4e8e\u5e73\u6ed1\u5e8f\u5217\uff0c\u4ece\u800c\u63d0\u53d6\u957f\u671f\u8d8b\u52bf\uff09</p> \\[ \\begin{array}{l} \\mathcal{X}_{\\mathrm{t}}=\\operatorname{AvgPool}(\\operatorname{Padding}(\\mathcal{X})) \\\\ \\mathcal{X}_{\\mathrm{s}}=\\mathcal{X}-\\mathcal{X}_{\\mathrm{t}}, \\end{array} \\]"},{"location":"DeepL/Paper/Autoformer/#encoder","title":"Encoder","text":"<p>\u7f16\u7801\u5668\u7684\u8f93\u5165\u662f\u8fc7\u53bb I \u4e2a\u65f6\u95f4\u7684\u6b65\u957f\u3002\u89e3\u7801\u5668\u7684\u8f93\u5165\u662f\u4f7f\u7528\u7f16\u7801\u5668\u8f93\u5165\u7684\u540e\u4e00\u534a\uff0c\u5e76\u5c06\u5176\u5206\u89e3\u4e3a\u8d8b\u52bf\u9879\u548c\u5b63\u8282\u9879\u3002</p> \\[ \\begin{aligned} \\mathcal{X}_{\\text {ens }}, \\mathcal{X}_{\\text {ent }} &amp; =\\operatorname{SeriesDecomp}\\left(\\mathcal{X}_{\\mathrm{en} \\frac{I}{2}: I}\\right) \\\\ \\mathcal{X}_{\\text {des }} &amp; =\\operatorname{Concat}\\left(\\mathcal{X}_{\\mathrm{ens}}, \\mathcal{X}_{0}\\right) \\\\ \\mathcal{X}_{\\text {det }} &amp; =\\operatorname{Concat}\\left(\\mathcal{X}_{\\mathrm{ent}}, \\mathcal{X}_{\\mathrm{Mean}}\\right) \\end{aligned} \\] <p>Encoder \u4e3b\u8981\u5173\u6ce8\u5b63\u8282\u9879\u90e8\u5206\uff0c\u5047\u8bbe\u6709 N \u5c42\u7f16\u7801\u5668\uff0c\u7b2c I \u5c42\u7f16\u7801\u5668\u5c42\uff1a\\(\\mathcal{X}_{\\mathrm{en}}^{l}=\\operatorname{Encoder}\\left(\\mathcal{X}_{\\mathrm{en}}^{l-1}\\right) .\\)\uff0c\u5177\u4f53\u6709\uff1a</p> \\[ \\begin{array}{l} \\mathcal{S}_{\\mathrm{en}}^{l, 1},_{-}=\\operatorname{SeriesDecomp}\\left(\\text { Auto-Correlation }\\left(\\mathcal{X}_{\\mathrm{en}}^{l-1}\\right)+\\mathcal{X}_{\\mathrm{en}}^{l-1}\\right) \\\\ \\mathcal{S}_{\\mathrm{en}}^{l, 2},_{-}=\\operatorname{SeriesDecomp}\\left(\\operatorname{FeedForward}\\left(\\mathcal{S}_{\\mathrm{en}}^{l, 1}\\right)+\\mathcal{S}_{\\mathrm{en}}^{l, 1}\\right), \\end{array} \\] <p>Encoder \u90e8\u5206\u4e3b\u8981\u76ee\u7684\u662f\u5bf9\u5e8f\u5217\u7684\u5b63\u8282\u9879\u8fdb\u884c\u5efa\u6a21\u3002\u901a\u8fc7\u591a\u5c42\u7684 Series Decomposition Block \u4e0d\u65ad\u4ece\u539f\u59cb\u5e8f\u5217\u4e2d\u63d0\u53d6\u5b63\u8282\u9879\uff0c\u6700\u7ec8\u4f1a\u6307\u5bfc Decoder \u5728\u672a\u6765\u9884\u6d4b\u5b63\u8282\u9879\u7684\u4fe1\u606f\u3002 </p>"},{"location":"DeepL/Paper/Autoformer/#decoder","title":"Decoder","text":"<p>Decoder \u5305\u542b\u4e24\u90e8\u5206\uff0c\u7528\u4e8e\u8d8b\u52bf\u6210\u5206\u7684\u7d2f\u8ba1\u7ed3\u6784\u548c\u7528\u4e8e\u5b63\u8282\u6210\u5206\u7684\u5806\u53e0\u81ea\u76f8\u5173\u673a\u5236\u3002</p> <p>\u5047\u8bbe\u89e3\u7801\u5668\u6709 \\(M\\) \u5c42\uff0c\u7f16\u7801\u5668\u7684\u8f93\u51fa\u4e3a \\(X^N_{en}\\)\uff0c\u5219\u7b2cI\u5c42\u7684\u89e3\u7801\u5668\u8f93\u51fa\u4e3a\\(X^i_{de}=Decoder(X^{i-1}_{de},X^N_{en})\\)\u3002</p> <p>\u7b2cI\u5c42\u5904\u7406\u6d41\u7a0b\u4e3a\uff1a</p> \\[ \\begin{aligned} \\mathcal{S}_{\\mathrm{de}}^{l, 1}, \\mathcal{T}_{\\mathrm{de}}^{l, 1} &amp; =\\operatorname{SeriesDecomp}\\left(\\text { Auto-Correlation }\\left(\\mathcal{X}_{\\mathrm{de}}^{l-1}\\right)+\\mathcal{X}_{\\mathrm{de}}^{l-1}\\right) \\\\ \\mathcal{S}_{\\mathrm{de}}^{l, 2}, \\mathcal{T}_{\\mathrm{de}}^{l, 2} &amp; =\\operatorname{SeriesDecomp}\\left(\\text { Auto-Correlation }\\left(\\mathcal{S}_{\\mathrm{de}}^{l, 1}, \\mathcal{X}_{\\mathrm{en}}^{N}\\right)+\\mathcal{S}_{\\mathrm{de}}^{l, 1}\\right) \\\\ \\mathcal{S}_{\\mathrm{de}}^{l, 3}, \\mathcal{T}_{\\mathrm{de}}^{l, 3} &amp; =\\operatorname{SeriesDecomp}\\left(\\text { FeedForward }\\left(\\mathcal{S}_{\\mathrm{de}}^{l, 2}\\right)+\\mathcal{S}_{\\mathrm{de}}^{l, 2}\\right) \\\\ \\mathcal{T}_{\\mathrm{de}}^{l} &amp; =\\mathcal{T}_{\\mathrm{de}}^{l-1}+\\mathcal{W}_{l, 1} * \\mathcal{T}_{\\mathrm{de}}^{l, 1}+\\mathcal{W}_{l, 2} * \\mathcal{T}_{\\mathrm{de}}^{l, 2}+\\mathcal{W}_{l, 3} * \\mathcal{T}_{\\mathrm{de}}^{l, 3}, \\end{aligned} \\] <ul> <li>\u5185\u90e8\u81ea\u76f8\u5173\u4e0e\u5e8f\u5217\u5206\u89e3\uff0c\u5c06\u4e0a\u4e00\u5c42\u8f93\u51fa\u4f5c\u4e3a\u8f93\u5165\uff0c\u5e94\u7528\u81ea\u76f8\u5173\u673a\u5236\uff0c\u7136\u540e\u505a\u6b8b\u5dee\u8fde\u63a5\u3002</li> <li>\u7f16\u7801\u5668-\u89e3\u7801\u5668\u81ea\u76f8\u5173\u4e0e\u5e8f\u5217\u5206\u89e3\u3002</li> <li>\u524d\u9988\u7f51\u7edc\u4e0e\u5e8f\u5217\u5206\u89e3\uff1a\u524d\u9988\u7f51\u7edc\u7528\u4e8e\u8fdb\u4e00\u6b65\u5904\u7406\u548c\u63d0\u53d6\u7279\u5f81\u3002</li> <li>\u8d8b\u52bf\u7d2f\u8ba1:\\(T_{d e}^{l}=T_{d e}^{l-1}+W_{l, 1} * T_{d e, 1}^{l}+W_{l, 2} * T_{d e, 2}^{l}+W_{l, 3} * T_{d e, 3}^{l}\\)</li> </ul> <p>\u6700\u7ec8\u9884\u6d4b\u7ed3\u679c\u4e3a\\(Ws * X^M_{de} + W_T * T^M_{de}\\)\u3002</p>"},{"location":"DeepL/Paper/Autoformer/#_3","title":"\u6a21\u578b\u5206\u6790","text":"<p>\u5c06Auto-Correlation\u4e0eself-attention family\u505a\u5bf9\u6bd4\uff0c\u5f53\u5e8f\u5217\u5f88\u957f\u7684\u65f6\u5019\uff0cAutoformer\u80fd\u591f\u6709\u66f4\u597d\u7684memory efficiency\u3002\u53e6\u5916\uff0c\u968f\u7740\u8f93\u5165\u957f\u5ea6\u589e\u5927\uff0c\u8f93\u51fa\u4e0d\u53d8\u65f6\uff0cMSE\u4e5f\u589e\u5927\uff0c\u53ef\u80fd\u8fc7\u4e45\u7684\u5386\u53f2\u6570\u636e\u88ab\u5f53\u4f5c\u4e86\u566a\u58f0\u3002</p> <p>\u968f\u7740\u5e8f\u5217\u5206\u89e3\u5355\u5143\u7684\u6570\u91cf\u589e\u52a0\uff0c\u6a21\u578b\u5b66\u5230\u7684\u8d8b\u52bf\u9879\u4f1a\u8d8a\u6765\u8d8a\u63a5\u8fd1\u771f\u5b9e\u7ed3\u679c\uff0c\u8fd9\u9a8c\u8bc1\u4e86\u6e10\u8fdb\u5f0f\u5206\u89e3\u7684\u4f5c\u7528\u3002</p> <p>\u5728\u663e\u5b58\u5360\u7528\u548c\u8fd0\u884c\u65f6\u95f4\u4e24\u4e2a\u6307\u6807\u4e0a\uff0c\u81ea\u76f8\u5173\u673a\u5236\u5747\u8868\u73b0\u51fa\u4e86\u4f18\u79c0\u7684\u7a7a\u95f4\u65f6\u95f4\u6548\u7387\uff0c\u4e24\u4e2a\u5c42\u9762\u5747\u8d85\u8fc7\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u8868\u73b0\u51fa\u9ad8\u6548\u7684\u590d\u6742\u5ea6\u3002</p>"},{"location":"DeepL/Paper/PatchTST/","title":"PatchTST","text":""},{"location":"DeepL/Paper/PatchTST/#_1","title":"\u95ee\u9898\u63cf\u8ff0","text":"<p>\u57fa\u4e8e Transformer \u7684\u9884\u6d4b\u6a21\u578b</p> <ul> <li>\u591a\u53d8\u91cf\u65f6\u5e8f\u9884\u6d4b</li> <li>\u81ea\u76d1\u7763\u8868\u793a\u5b66\u4e60</li> </ul> <p>channel-independence patch time series Transformer(PatchTST)</p> <ul> <li>\u5c06\u65f6\u95f4\u6b65\u805a\u5408\u4e3a\u5b50\u5e8f\u5217\u7ea7\u522b\u7684 Patch</li> <li>\u901a\u9053\u72ec\u7acb\uff1a\u6bcf\u4e2a\u901a\u9053\u5305\u542b\u5355\u4e2a\u7279\u5f81\u7684\u65f6\u95f4\u5e8f\u5217</li> </ul> <p>\u6a21\u578b\u4f18\u52bf</p> <ul> <li>\u65f6\u95f4\u548c\u7a7a\u95f4\u590d\u6742\u5ea6\u7684\u964d\u4f4e\uff1a\\(N=L \\to N \\approx \\frac{L}{s}\\)</li> <li>\u53ef\u4ee5\u5173\u6ce8\u66f4\u957f\u7684\u5386\u53f2\u5e8f\u5217</li> <li>\u8868\u793a\u5b66\u4e60\u7684\u80fd\u529b\uff1a\u6355\u83b7\u6570\u636e\u7684\u62bd\u8c61\u8868\u793a</li> </ul>"},{"location":"DeepL/Paper/PatchTST/#_2","title":"\u6a21\u578b\u5b9e\u73b0","text":"<p>\u5173\u952e\u53c2\u6570</p> <ul> <li>Patch \u957f\u5ea6\uff08P\uff09</li> <li>Stride \u6b65\u957f\uff08S\uff09\uff1apatch \u53ef\u4ee5\u91cd\u53e0\u6216\u975e\u91cd\u53e0</li> <li>Token \u4e2a\u6570\uff08N\uff09\uff1a\\(N=\\left\\lfloor\\frac{(L-P)}{S}\\right\\rfloor+2\\)</li> </ul>"},{"location":"DeepL/Paper/PatchTST/#_3","title":"\u8868\u793a\u5b66\u4e60","text":"<p>\u9884\u8bad\u7ec3\u8f85\u52a9\u4efb\u52a1</p> <ul> <li>\u5c06\u6bcf\u4e2a\u8f93\u5165\u5e8f\u5217\u5212\u5206\u4e3a\u4e0d\u91cd\u53e0\u7684 patch</li> <li>\u968f\u673a\u63a9\u7801\uff0c\u906e\u853d\u6570\u636e\u5757</li> <li>\u91cd\u5efa\u88ab\u63a9\u7801\u7684 patch</li> </ul> <p>\u89e3\u51b3\u7684\u95ee\u9898</p> <ul> <li>\u63a9\u76d6\u6570\u636e\u70b9\uff0c\u63d2\u503c\u5bfc\u81f4\u6a21\u578b\u65e0\u6cd5\u5b66\u4e60\u5230\u91cd\u8981\u7684\u62bd\u8c61\u8868\u793a</li> </ul>"},{"location":"DeepL/Paper/PatchTST/#1","title":"\u5b9e\u9a8c 1\uff1a\u957f\u65f6\u9884\u6d4b","text":"<p>\u56de\u671b\u7a97\u53e3\uff1aAutoformer\uff0cFEDformer\uff0cInformer\uff0cL = [24,48,96,192,336,720] \u9009\u6700\u4f73\uff0cDLinear\uff1aL = 336</p> <p>PatchTST\uff1ap = 16\uff0cs = 8\uff1bPatchTST/64\uff1aL = 512, N = 64\uff1bPatchTST/42\uff1aL = 336\uff0cN = 42</p>"},{"location":"DeepL/Paper/PatchTST/#2-","title":"\u5b9e\u9a8c 2\uff1a\u8868\u793a\u5b66\u4e60-\u9884\u6d4b\u4efb\u52a1","text":"<p>Patch \u4e0d\u91cd\u53e0\uff0cL = 512\uff0cP = 12\uff0c\u63a9\u7801\u7387 40%</p> <p>\u5728\u7535\u529b\u6570\u636e\u96c6\u4e0a\u505a\u9884\u8bad\u7ec3\uff0c\u5728\u5176\u4ed6\u6570\u636e\u96c6\u4e0a\u5fae\u8c03</p>"},{"location":"DeepL/Paper/PatchTST/#_4","title":"\u6d88\u878d\u5b9e\u9a8c","text":"<p>CI\uff1a\u53ea\u6709\u901a\u9053\u72ec\u7acb\uff1bP\uff1a\u53ea\u6709patch</p> <p>\u672c\u6587patch\u957f\u5ea6\u8bbe\u5b9a\u4e3a16</p>"},{"location":"DeepL/Paper/Transformer/","title":"Transformer \u65f6\u5e8f\u9884\u6d4b pytorch \u5b9e\u73b0","text":""},{"location":"DeepL/Paper/Transformer/#dateembedding","title":"DateEmbedding","text":"<p>\u539f\u59cb\u7684token\uff08\u5355\u8bcd\u3001\u5b57\u8bcd\u3001\u65f6\u95f4\u7247\u6bb5\u7684\u7d22\u5f15\uff09\u662f\u7eaf\u7cb9\u7684\u6574\u6570\u7d22\u5f15\uff0c\u6ca1\u6709\u4efb\u4f55\u8bed\u4e49\u4e0a\u7684\u8fde\u7eed\u4fe1\u606f\u3002\u4e3a\u4e86\u8ba9\u6a21\u578b\u7406\u89e3\u5e76\u5904\u7406\u8fd9\u4e9b\u79bb\u6563\u7b26\u53f7\uff0c\u9700\u8981\u5c06\u5176\u6620\u5c04\u5230\u4e00\u4e2a\u8fde\u7eed\u5411\u91cf\u7a7a\u95f4\u4e2d\uff0c\u5373\u901a\u8fc7 <code>tokenEmbedding</code>\uff08\u8bcd\u5d4c\u5165\uff09\u5c06\u79bb\u6563\u7684 token \u8f6c\u5316\u4e3a\u53ef\u8bad\u7ec3\u7684\u5bc6\u96c6\u5411\u91cf\u8868\u793a\u3002</p> <p><code>TimeFeatureEmbedding</code> \u7684\u4f5c\u7528\u662f\u5bf9\u65f6\u95f4\u7279\u5f81\uff08\u5982\u6708\u3001\u65e5\u3001\u5468\u3001\u5c0f\u65f6\u7b49\uff09\u8fdb\u884c\u5d4c\u5165\uff08embedding\uff09\u5904\u7406\u3002\u4e0e\u4f20\u7edf\u7684<code>tokenEmbedding</code>\u4e0d\u540c\uff0c\u8fd9\u91cc\u4e0d\u662f\u5bf9\u79bb\u6563\u7684\u8bcd\u6216\u6807\u8bb0\u8fdb\u884c\u5d4c\u5165\uff0c\u800c\u662f\u9488\u5bf9\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u6bcf\u4e2a\u65f6\u95f4\u70b9\u7684\u65f6\u95f4\u7279\u5f81\uff08\u7531\u65f6\u95f4\u6233\u63d0\u53d6\u51fa\u7684\u6570\u503c\uff0c\u5982\u6708\u4efd\u3001\u661f\u671f\u51e0\u3001\u5c0f\u65f6\u6570\u7b49\uff09\u3002</p> <p><code>PositionalEmbedding</code>\uff08\u4f4d\u7f6e\u5d4c\u5165\uff09\u7684\u4f5c\u7528\u662f\u4e3a\u5e8f\u5217\u6570\u636e\u4e2d\u7684\u6bcf\u4e2a\u4f4d\u7f6e\uff08\u5982\u8bcd\u5728\u53e5\u5b50\u4e2d\u7684\u4f4d\u7f6e\u3001\u65f6\u95f4\u6b65\u5728\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u4f4d\u7f6e\uff09\u63d0\u4f9b\u4f4d\u7f6e\u4fe1\u606f\uff0c\u4f7f\u6a21\u578b\u53ef\u4ee5\u533a\u5206\u5e8f\u5217\u4e2d\u4e0d\u540c\u5143\u7d20\u7684\u987a\u5e8f\u548c\u76f8\u5bf9\u5173\u7cfb\u3002</p> <p>\u5728 Transformer \u8fd9\u7c7b\u4f7f\u7528\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u6a21\u578b\u4e2d\uff0c\u6ce8\u610f\u529b\u673a\u5236\u672c\u8eab\u5e76\u6ca1\u6709\u987a\u5e8f\u611f\u3002\u4e5f\u5c31\u662f\u8bf4\uff0c\u5982\u679c\u4ec5\u6709 token \u7684\u5d4c\u5165\u8868\u793a\uff0c\u6a21\u578b\u53ea\u770b\u5230\u4e86\u5143\u7d20\u7684\u5185\u5bb9\u8868\u793a\uff08\u5982\u8bcd\u5411\u91cf\uff09\uff0c\u800c\u4e0d\u77e5\u9053\u8fd9\u4e9b\u5143\u7d20\u5728\u8f93\u5165\u5e8f\u5217\u4e2d\u7684\u987a\u5e8f\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u9700\u8981\u4e3a\u6bcf\u4e2a token \u6216\u6570\u636e\u70b9\u6dfb\u52a0\u5bf9\u5e94\u7684\u4f4d\u7f6e\u4fe1\u606f\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u611f\u77e5\u987a\u5e8f\u548c\u8ddd\u79bb\u3002</p> Python<pre><code>class DataEmbedding(nn.Module):\n    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):\n        super(DataEmbedding, self).__init__()\n\n        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n        self.position_embedding = PositionalEmbedding(d_model=d_model)\n        self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type,\n                                                    freq=freq) if embed_type != 'timeF' else TimeFeatureEmbedding(\n            d_model=d_model, embed_type=embed_type, freq=freq)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, x, x_mark):\n        x = self.value_embedding(x) + self.temporal_embedding(x_mark) + self.position_embedding(x)\n        return self.dropout(x)\n</code></pre>"},{"location":"DeepL/Paper/Transformer/#positionalembedding","title":"PositionalEmbedding","text":"Python<pre><code>class PositionalEmbedding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super(PositionalEmbedding, self).__init__()\n        # Compute the positional encodings once in log space.\n        pe = torch.zeros(max_len, d_model).float()\n        pe.require_grad = False\n\n        position = torch.arange(0, max_len).float().unsqueeze(1)\n        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        return self.pe[:, :x.size(1)]\n</code></pre>"},{"location":"DeepL/Paper/Transformer/#tokenembedding","title":"TokenEmbedding","text":"<p>\u91c7\u7528\u4e00\u7ef4\u5377\u79ef\u5b9e\u73b0\uff0cencoder\uff1a<code>[32,96,7]-&gt;[32,96,512]</code>\uff0cdecoder\uff1a<code>[32,72,7]-&gt;[32,72,512]</code></p> Python<pre><code>class TokenEmbedding(nn.Module):\n    def __init__(self, c_in, d_model):\n        super(TokenEmbedding, self).__init__()\n        padding = 1 if compared_version(torch.__version__, '1.5.0') else 2\n        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model,\n                                   kernel_size=3, padding=padding, padding_mode='circular', bias=False)\n        for m in self.modules():\n            if isinstance(m, nn.Conv1d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n\n    def forward(self, x):\n        x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)\n        return x\n</code></pre>"},{"location":"DeepL/Paper/Transformer/#timefeatureembedding","title":"TimeFeatureEmbedding","text":"<p>\u5728ETTh\u6570\u636e\u4e2d\uff0c\u5c06\u65f6\u95f4\u62c6\u5206\u4e3a4\u4e2a\u7ef4\u5ea6\uff1a\u6708\u3001\u5929\u3001\u5468\u4ee5\u53ca\u5c0f\u65f6\u3002<code>[32,96\\72,4]-&gt;[32,96\\72,512]</code></p> Python<pre><code>class TimeFeatureEmbedding(nn.Module):\n    def __init__(self, d_model, embed_type='timeF', freq='h'):\n        super(TimeFeatureEmbedding, self).__init__()\n\n        freq_map = {'h': 4, 't': 5, 's': 6, 'm': 1, 'a': 1, 'w': 2, 'd': 3, 'b': 3}\n        d_inp = freq_map[freq]\n        self.embed = nn.Linear(d_inp, d_model, bias=False)\n\n    def forward(self, x):\n        return self.embed(x)\n</code></pre>"},{"location":"DeepL/Paper/Transformer/#attentionlayer","title":"AttentionLayer","text":"Python<pre><code>class AttentionLayer(nn.Module):\n    def __init__(self, attention, d_model, n_heads, d_keys=None,\n                 d_values=None):\n        super(AttentionLayer, self).__init__()\n\n        d_keys = d_keys or (d_model // n_heads)\n        d_values = d_values or (d_model // n_heads)\n\n        self.inner_attention = attention\n        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n        self.key_projection = nn.Linear(d_model, d_keys * n_heads)\n        self.value_projection = nn.Linear(d_model, d_values * n_heads)\n        self.out_projection = nn.Linear(d_values * n_heads, d_model)\n        self.n_heads = n_heads\n\n    def forward(self, queries, keys, values, attn_mask):\n        B, L, _ = queries.shape\n        _, S, _ = keys.shape\n        H = self.n_heads\n\n        queries = self.query_projection(queries).view(B, L, H, -1)\n        keys = self.key_projection(keys).view(B, S, H, -1)\n        values = self.value_projection(values).view(B, S, H, -1)\n\n        out, attn = self.inner_attention(\n            queries,\n            keys,\n            values,\n            attn_mask\n        )\n        out = out.view(B, L, -1)\n\n        return self.out_projection(out), attn\n</code></pre>"},{"location":"DeepL/Paper/Transformer/#fullattention","title":"FullAttention","text":"Python<pre><code>class FullAttention(nn.Module):\n    def __init__(self, mask_flag=True, factor=5, scale=None, attention_dropout=0.1, output_attention=False):\n        super(FullAttention, self).__init__()\n        self.scale = scale\n        self.mask_flag = mask_flag\n        self.output_attention = output_attention\n        self.dropout = nn.Dropout(attention_dropout)\n\n    def forward(self, queries, keys, values, attn_mask):\n        B, L, H, E = queries.shape\n        _, S, _, D = values.shape\n        scale = self.scale or 1. / sqrt(E)\n\n        scores = torch.einsum(\"blhe,bshe-&gt;bhls\", queries, keys)\n\n        if self.mask_flag:\n            if attn_mask is None:\n                attn_mask = TriangularCausalMask(B, L, device=queries.device)\n\n            scores.masked_fill_(attn_mask.mask, -np.inf)\n\n        A = self.dropout(torch.softmax(scale * scores, dim=-1))\n        V = torch.einsum(\"bhls,bshd-&gt;blhd\", A, values)\n\n        if self.output_attention:\n            return (V.contiguous(), A)\n        else:\n            return (V.contiguous(), None)\n</code></pre>"},{"location":"DeepL/Paper/Transformer/#encoder","title":"Encoder","text":"<p><code>Encoder</code> \u662f\u7531\u591a\u4e2a <code>EncoderLayer</code> \u7ec4\u6210\u7684\u6a21\u5757\u5217\u8868\u3002\u6bcf\u4e2a <code>EncoderLayer</code> \u5305\u542b\u4e00\u4e2a <code>AttentionLayer</code> \u548c\u524d\u9988\u7f51\u7edc\uff08Feed-Forward Network\uff09\uff0c\u5e76\u901a\u8fc7\u5806\u53e0\u591a\u4e2a <code>EncoderLayer</code>\uff0c<code>Encoder</code> \u80fd\u591f\u9010\u5c42\u5904\u7406\u8f93\u5165\u6570\u636e\uff0c\u9010\u6b65\u63d0\u53d6\u66f4\u9ad8\u7ea7\u7684\u7279\u5f81\u3002</p> Python<pre><code>class Encoder(nn.Module):\n    def __init__(self, attn_layers, conv_layers=None, norm_layer=None):\n        super(Encoder, self).__init__()\n        self.attn_layers = nn.ModuleList(attn_layers)\n        self.conv_layers = nn.ModuleList(conv_layers) if conv_layers is not None else None\n        self.norm = norm_layer\n\n    def forward(self, x, attn_mask=None):\n        # x [B, L, D]\n        attns = []\n        if self.conv_layers is not None:\n            for attn_layer, conv_layer in zip(self.attn_layers, self.conv_layers):\n                x, attn = attn_layer(x, attn_mask=attn_mask)\n                x = conv_layer(x)\n                attns.append(attn)\n            x, attn = self.attn_layers[-1](x)\n            attns.append(attn)\n        else:\n            for attn_layer in self.attn_layers:\n                x, attn = attn_layer(x, attn_mask=attn_mask)\n                attns.append(attn)\n\n        if self.norm is not None:\n            x = self.norm(x)\n\n        return x, attns\n</code></pre>"},{"location":"DeepL/Paper/Transformer/#encoder_layer","title":"Encoder_layer","text":"<p><code>EncoderLayer</code> \u7c7b\uff1a\u8868\u793a\u7f16\u7801\u5668\u4e2d\u7684\u5355\u4e2a\u5c42\uff0c\u6bcf\u4e00\u5c42\u5305\u542b\u4e00\u4e2a\u6ce8\u610f\u529b\u673a\u5236\uff08<code>AttentionLayer</code>\uff09\u548c\u4e00\u4e2a\u524d\u9988\u795e\u7ecf\u7f51\u7edc\uff08\u901a\u8fc7\u4e24\u4e2a\u5377\u79ef\u5c42\u5b9e\u73b0\uff09\u3002\u6bcf\u4e2a <code>EncoderLayer</code> \u8d1f\u8d23\u5bf9\u8f93\u5165\u8fdb\u884c\u4e00\u6b21\u81ea\u6ce8\u610f\u529b\u8ba1\u7b97\u548c\u4e00\u6b21\u975e\u7ebf\u6027\u53d8\u6362\uff0c\u4ece\u800c\u9010\u6b65\u63d0\u53d6\u8f93\u5165\u7684\u9ad8\u7ea7\u7279\u5f81\u3002</p> Python<pre><code>class EncoderLayer(nn.Module):\n    def __init__(self, attention, d_model, d_ff=None, dropout=0.1, activation=\"relu\"):\n        super(EncoderLayer, self).__init__()\n        d_ff = d_ff or 4 * d_model\n        self.attention = attention\n        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n        self.activation = F.relu if activation == \"relu\" else F.gelu\n\n    def forward(self, x, attn_mask=None):\n        new_x, attn = self.attention(\n            x, x, x,\n            attn_mask=attn_mask\n        )\n        x = x + self.dropout(new_x)\n\n        y = x = self.norm1(x)\n        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n        y = self.dropout(self.conv2(y).transpose(-1, 1))\n\n        return self.norm2(x + y), attn\n</code></pre>"},{"location":"DeepL/Paper/iTransformer/","title":"iTransformer","text":""},{"location":"DeepL/Paper/iTransformer/#_1","title":"\u7814\u7a76\u95ee\u9898","text":"<p>Transformer\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u5c40\u9650\u6027\uff1a\u867d\u7136Transformer\u5df2\u7ecf\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u53d6\u5f97\u4e86\u4e00\u5b9a\u7684\u6210\u529f\uff0c\u4f46\u5728\u9762\u5bf9\u8f83\u957f\u56de\u6eaf\u7a97\u53e3\u65f6\uff0c\u5176\u6027\u80fd\u5f80\u5f80\u4f1a\u51fa\u73b0\u4e0b\u964d\uff0c\u5e76\u4e14\u8ba1\u7b97\u5f00\u9500\u663e\u8457\u589e\u52a0\u3002\u6b64\u5916\uff0cTransformer\u5c06\u591a\u4e2a\u4e0d\u540c\u7684\u53d8\u91cf\uff08\u5982\u6f5c\u5728\u5ef6\u8fdf\u4e8b\u4ef6\u6216\u4e0d\u540c\u7684\u7269\u7406\u6d4b\u91cf\uff09\u5d4c\u5165\u5230\u540c\u4e00\u4e2a\u65f6\u95f4\u6233\u7684\u65f6\u95f4\u70b9\u4e2d\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u65e0\u6cd5\u6709\u6548\u5b66\u4e60\u53d8\u91cf\u4e2d\u5fc3\u7684\u8868\u793a\uff0c\u8fdb\u800c\u4ea7\u751f\u65e0\u610f\u4e49\u7684\u6ce8\u610f\u529b\u56fe\u3002</p> <p>\u5982\u4f55\u6709\u6548\u5229\u7528\u65f6\u95f4\u5e8f\u5217\u7684\u591a\u53d8\u91cf\u7279\u5f81\uff1a\u4f20\u7edf\u7684Transformer\u6a21\u578b\u53ef\u80fd\u65e0\u6cd5\u5904\u7406\u591a\u53d8\u91cf\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u5c24\u5176\u662f\u5728\u6bcf\u4e2a\u65f6\u95f4\u6233\u5305\u542b\u591a\u4e2a\u53d8\u91cf\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u4fdd\u6301\u8fd9\u4e9b\u53d8\u91cf\u7684\u72ec\u7acb\u6027\uff0c\u5e76\u6709\u6548\u6355\u6349\u5b83\u4eec\u4e4b\u95f4\u7684\u76f8\u4e92\u5173\u7cfb\u662f\u4e00\u4e2a\u6311\u6218\u3002</p> <p>Transformer\u67b6\u6784\u7684\u6539\u8fdb\uff1a\u5728\u4e0d\u4fee\u6539Transformer\u7684\u57fa\u672c\u7ec4\u4ef6\uff08\u5982\u6ce8\u610f\u529b\u673a\u5236\u548c\u524d\u9988\u7f51\u7edc\uff09\u7684\u524d\u63d0\u4e0b\uff0c\u5982\u4f55\u91cd\u65b0\u7ec4\u7ec7\u548c\u5e94\u7528\u8fd9\u4e9b\u7ec4\u4ef6\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u5728\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002</p> <p></p>"},{"location":"DeepL/Paper/iTransformer/#_2","title":"\u7814\u7a76\u73b0\u72b6","text":""},{"location":"DeepL/Paper/iTransformer/#_3","title":"\u521b\u65b0\u70b9","text":"<p>\u63d0\u51faiTransformer\u6a21\u578b\uff1aiTransformer\u901a\u8fc7\u5c06\u6bcf\u4e2a\u65f6\u95f4\u5e8f\u5217\u89c6\u4e3a\u72ec\u7acb\u7684\u53d8\u91cf\uff0c\u5229\u7528\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6355\u6349\u4e0d\u540c\u53d8\u91cf\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002\u540c\u65f6\uff0c\u4f7f\u7528\u5c42\u5f52\u4e00\u5316\u548c\u524d\u9988\u7f51\u7edc\u6a21\u5757\u6765\u5b66\u4e60\u5168\u5c40\u7684\u65f6\u95f4\u5e8f\u5217\u8868\u793a\u3002\u4e0e\u4f20\u7edfTransformer\u4e0d\u540c\uff0ciTransformer\u4e0d\u5c06\u591a\u4e2a\u53d8\u91cf\u7684\u7279\u5f81\u5d4c\u5165\u5230\u540c\u4e00\u4e2a\u65f6\u95f4\u6233\u7684\u5355\u4e00\u8868\u793a\u4e2d\uff0c\u800c\u662f\u5355\u72ec\u5904\u7406\u6bcf\u4e2a\u53d8\u91cf\uff0c\u4ee5\u66f4\u597d\u5730\u6355\u6349\u5b83\u4eec\u7684\u76f8\u4e92\u5173\u7cfb\u3002</p>"},{"location":"DeepL/Paper/iTransformer/#_4","title":"\u6a21\u578b\u6846\u67b6","text":"<p>\u4f7f\u7528Encoder-Only</p> <ul> <li> <p>\u7b80\u5316\u7ed3\u6784\uff1a\u4f20\u7edf\u7684Encoder-Decoder\u7ed3\u6784\u901a\u5e38\u5728\u5904\u7406\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u65f6\u9700\u8981\u989d\u5916\u7684\u89e3\u7801\u5668\u90e8\u5206\u6765\u751f\u6210\u9884\u6d4b\u7ed3\u679c\uff0c\u8fd9\u589e\u52a0\u4e86\u6a21\u578b\u7684\u590d\u6742\u6027\u548c\u8ba1\u7b97\u5f00\u9500\u3002\u800ciTransformer\u901a\u8fc7\u4ec5\u4f7f\u7528Encoder\u90e8\u5206\uff0c\u7b80\u5316\u4e86\u6a21\u578b\u7ed3\u6784\uff0c\u540c\u65f6\u4fdd\u7559\u4e86Transformer\u7684\u6838\u5fc3\u4f18\u52bf\u2014\u2014\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6355\u6349\u957f\u65f6\u95f4\u4f9d\u8d56\u548c\u591a\u53d8\u91cf\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002</p> </li> <li> <p>\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff1a\u5728iTransformer\u4e2d\uff0c\u6bcf\u4e2a\u65f6\u95f4\u5e8f\u5217\u88ab\u89c6\u4e3a\u72ec\u7acb\u7684\u53d8\u91cf\uff08variate tokens\uff09\uff0c\u5e76\u901a\u8fc7Encoder\u90e8\u5206\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6765\u6355\u6349\u8fd9\u4e9b\u53d8\u91cf\u4e4b\u95f4\u7684\u591a\u7ef4\u76f8\u5173\u6027\u3002\u7531\u4e8e\u53bb\u9664\u4e86Decoder\u90e8\u5206\uff0c\u6a21\u578b\u7684\u8ba1\u7b97\u8d1f\u62c5\u51cf\u5c11\uff0c\u80fd\u591f\u66f4\u9ad8\u6548\u5730\u5904\u7406\u957f\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u907f\u514d\u4e86Transformer\u5728\u957f\u56de\u6eaf\u7a97\u53e3\u4e0b\u7684\u8ba1\u7b97\u7206\u70b8\u95ee\u9898\u3002</p> </li> <li> <p>\u4e13\u6ce8\u4e8e\u5e8f\u5217\u8868\u793a\uff1a\u901a\u8fc7\u53ea\u4f7f\u7528Encoder\uff0ciTransformer\u4e13\u6ce8\u4e8e\u5b66\u4e60\u8f93\u5165\u65f6\u95f4\u5e8f\u5217\u7684\u5168\u5c40\u8868\u793a\u3002\u8fd9\u79cd\u65b9\u6cd5\u8ba9\u6a21\u578b\u80fd\u591f\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6355\u6349\u5230\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u591a\u53d8\u91cf\u5173\u7cfb\uff0c\u800c\u65e0\u9700\u89e3\u7801\u5668\u6765\u8fdb\u884c\u540e\u7eed\u7684\u751f\u6210\u4efb\u52a1\u3002</p> </li> <li> <p>\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\uff1a\u4f7f\u7528Encoder-only\u7ed3\u6784\u4f7f\u5f97\u6a21\u578b\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b\u4e2d\u66f4\u52a0\u9ad8\u6548\uff0c\u540c\u65f6\u53ef\u4ee5\u66f4\u597d\u5730\u6cdb\u5316\u5230\u4e0d\u540c\u7684\u65f6\u95f4\u5e8f\u5217\u4efb\u52a1\u548c\u4e0d\u540c\u7684\u56de\u6eaf\u7a97\u53e3\u4e0a\u3002</p> </li> </ul> <p>\u5728\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\uff0c\u7ed9\u5b9a\u5386\u53f2\u89c2\u6d4b\u503c \\(X = \\{x_1, \\dots, x_T\\} \\in \\mathbb{R}^{T \\times N}\\)\uff0c\u5176\u4e2d T \u662f\u65f6\u95f4\u6b65\u957f\uff0cN \u662f\u53d8\u91cf\u7684\u6570\u91cf\uff0c\u6211\u4eec\u9884\u6d4b\u672a\u6765 S \u4e2a\u65f6\u95f4\u6b65\u7684\u503c \\(Y = \\{x_{T+1}, \\dots, x_{T+S}\\} \\in \\mathbb{R}^{S \\times N}\\)\u3002\u4e3a\u65b9\u4fbf\u8d77\u89c1\uff0c\u6211\u4eec\u5c06 \\(X_{t,:}\\)\u8868\u793a\u4e3a\u5728\u65f6\u95f4\u6b65 t \u540c\u65f6\u8bb0\u5f55\u7684\u6240\u6709\u65f6\u95f4\u70b9\uff0c\u800c \\(X_{:,n}\\) \u8868\u793a\u6bcf\u4e2a\u53d8\u91cf n \u7684\u5b8c\u6574\u65f6\u95f4\u5e8f\u5217\u3002\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\\(X_{t,:}\\)\u4e2d\u7684\u65f6\u95f4\u70b9\u53ef\u80fd\u5e76\u4e0d\u53cd\u6620\u73b0\u5b9e\u4e16\u754c\u4e2d\u540c\u4e00\u4e8b\u4ef6\u7684\u65f6\u95f4\uff0c\u56e0\u4e3a\u6570\u636e\u96c6\u4e2d\u7684\u5404\u4e2a\u53d8\u91cf\u4e4b\u95f4\u53ef\u80fd\u5b58\u5728\u7cfb\u7edf\u6027\u7684\u65f6\u95f4\u5ef6\u8fdf\u3002\u6b64\u5916\uff0c\\(X_{t,:}\\)\u4e2d\u7684\u5143\u7d20\u5728\u7269\u7406\u6d4b\u91cf\u548c\u7edf\u8ba1\u5206\u5e03\u4e0a\u53ef\u80fd\u662f\u4e0d\u540c\u7684\uff0c\u800c\u6bcf\u4e2a\u53d8\u91cf \\(X_{:,n}\\)\u4e00\u822c\u5171\u4eab\u8fd9\u4e9b\u7279\u5f81\u3002</p> <p>\u5728iTransformer\u4e2d\uff0c\u6bcf\u4e2a\u7279\u5b9a\u53d8\u91cf\u7684\u672a\u6765\u5e8f\u5217\u9884\u6d4b \\(\\hat{Y}_{:,n}\\),n \u662f\u57fa\u4e8e\u56de\u987e\u5e8f\u5217 \\(X_{:,n}\\)\u7b80\u5355\u5730\u8868\u8ff0\u4e3a\u5982\u4e0b\u8fc7\u7a0b\uff1a</p> \\[ \\begin{aligned} \\mathbf{h}_{n}^{0} &amp; =\\operatorname{Embedding}\\left(\\mathbf{X}_{:, n}\\right), \\\\ \\mathbf{H}^{l+1} &amp; =\\operatorname{TrmBlock}\\left(\\mathbf{H}^{l}\\right), l=0, \\cdots, L-1, \\\\ \\hat{\\mathbf{Y}}_{:, n} &amp; =\\operatorname{Projection}\\left(\\mathbf{h}_{n}^{L}\\right), \\end{aligned} \\] <p>\u5176\u4e2d \\(H = \\{h_1, \\cdots, h_N\\} \\in \\mathbb{R}^{N \\times D}\\)\u5305\u542b\u4e86 N \u4e2a\u7ef4\u5ea6\u4e3a D \u7684\u5d4c\u5165 token\uff0c\u6307\u6570\u8868\u793a\u5c42\u7684\u7d22\u5f15\u3002\u5f97\u5230\u7684\u53d8\u91cf token \u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u673a\u5236\u76f8\u4e92\u4f5c\u7528\uff0c\u5e76\u5728\u6bcf\u4e2a TrmBlock \u4e2d\u7531\u5171\u4eab\u7684\u524d\u9988\u7f51\u7edc\u72ec\u7acb\u5904\u7406\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u7531\u4e8e\u5e8f\u5217\u7684\u987a\u5e8f\u9690\u5f0f\u5730\u5b58\u50a8\u5728\u524d\u9988\u7f51\u7edc\u795e\u7ecf\u5143\u7684\u6392\u5217\u4e2d\uff0c\u56e0\u6b64\u5728\u8fd9\u91cc\u4e0d\u518d\u9700\u8981\u4f7f\u7528\u4f20\u7edf Transformer \u4e2d\u7684\u4f4d\u7f6e\u4fe1\u606f\u7f16\u7801\u3002</p>"},{"location":"DeepL/Paper/iTransformer/#_5","title":"\u5b9e\u9a8c\u7ed3\u679c","text":""},{"location":"DeepL/Paper/informer/","title":"Informer","text":""},{"location":"DeepL/Paper/informer/#_1","title":"\u76ee\u524d\u7684\u95ee\u9898\u548c\u5df2\u6709\u89e3\u51b3\u65b9\u6cd5","text":"<p>Vanilla Transformer \u5728\u89e3\u51b3 LSTF \u95ee\u9898\u65f6\u5b58\u5728\u4e09\u4e2a\u663e\u8457\u9650\u5236\uff1a</p> <ul> <li>Self-Attention \u5e73\u65b9\u7ea7\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u3002</li> <li>\u5806\u53e0\u591a\u5c42\u7f51\u7edc\uff0c\u5185\u5b58\u5360\u7528\u74f6\u9888\u3002</li> <li>step-by-step \u89e3\u7801\u9884\u6d4b\uff0c\u901f\u5ea6\u8f83\u6162\u3002</li> </ul> <p>\u6709\u4e00\u4e9b\u5173\u4e8e\u63d0\u9ad8\u81ea\u6ce8\u610f\u529b\u6548\u7387\u7684\u5148\u524d\u5de5\u4f5c\uff1a</p> <ul> <li>Sparse Transformer</li> <li>LogSparse Transformer</li> <li>Longformer</li> </ul> <p>\u90fd\u4f7f\u7528\u542f\u53d1\u5f0f\u65b9\u6cd5\u6765\u89e3\u51b3\u9650\u5236 1\uff0c\u5e76\u5c06 Self-Attention \u673a\u5236\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u964d\u4f4e\u5230 \\(O(LlogL)\\)\uff0c\u4f46\u95ee\u9898\u5728\u4e8e\u5b83\u4eec\u7684\u6548\u7387\u589e\u76ca\u662f\u6709\u9650\u7684\u3002</p>"},{"location":"DeepL/Paper/informer/#informer_1","title":"Informer \u6539\u8fdb","text":"<ul> <li>\u63d0\u51fa ProbSparse Self-Attention\uff0c\u7b5b\u9009\u51fa\u6700\u91cd\u8981\u7684 query\uff0c\u4f7f\u590d\u6742\u5ea6\u964d\u4f4e\u5230 \\(O(LlogL)\\)</li> <li>\u63d0\u51fa Self-Attention Distilling\uff0c\u51cf\u5c11\u7ef4\u5ea6\u548c\u7f51\u7edc\u53c2\u6570\u91cf\u3002</li> <li>\u63d0\u51fa Generative Style Decoder\uff0c\u4e00\u6b65\u5f97\u5230\u6240\u6709\u9884\u6d4b\u7ed3\u679c\u3002</li> </ul> <p>\u5de6\u56fe\u5c55\u793a\u4e86\u4e0e\u77ed\u671f\u9884\u6d4b\u76f8\u6bd4\uff0cLTSF \u53ef\u4ee5\u9884\u6d4b\u66f4\u957f\u7684\u5e8f\u5217\uff1b\u53f3\u56fe\u8868\u660e\u968f\u7740\u9884\u6d4b\u5e8f\u5217\u957f\u5ea6\u589e\u52a0\uff0c\u4ece \\(L=48\\) \u5f00\u59cb\uff0cMSE \u8fc5\u901f\u589e\u5927\u63a8\u7406\u901f\u5ea6\u4e0b\u964d\u3002</p>"},{"location":"DeepL/Paper/informer/#informer_2","title":"Informer \u67b6\u6784","text":"<ul> <li>Encoder \u63a5\u53d7\u5927\u91cf\u957f\u5e8f\u5217\u8f93\u5165\u3002\u6a21\u578b\u91c7\u7528\u4e86 ProbSparse Self-Attention \u4ee3\u66ff\u4e86 Transformer \u4e2d\u7684 Self-Attention\u3002\u5e76\u4e14 Encoder \u5728\u5806\u53e0\u65f6\u91c7\u7528\u4e86 Self-Attention Distilling\u3002</li> <li>Decoder \u540c\u6837\u63a5\u53d7\u957f\u5e8f\u5217\u8f93\u5165\uff0c\u9884\u6d4b\u90e8\u5206\u7528 0 \u8fdb\u884c padding\u3002\u7ed3\u679c\u5904\u7406\u540e\u76f4\u63a5\u8f93\u51fa\u6240\u6709\u9884\u6d4b\u7ed3\u679c\u3002</li> </ul> Python<pre><code># e_layer = 3, d_layer = 2\nInformer(\n  # encoder embedding\uff0c\u7f16\u7801\u5668\u7aef\u7684embedding\n  (enc_embedding): DataEmbedding(\n    (value_embedding): TokenEmbedding(\n      (tokenConv): Conv1d(7, 512, kernel_size=(3,), stride=(1,), padding=(1,), padding_mode=circular)\n    )\n    (position_embedding): PositionalEmbedding()\n    (temporal_embedding): TimeFeatureEmbedding(\n      (embed): Linear(in_features=4, out_features=512, bias=True)\n    )\n    (dropout): Dropout(p=0.05, inplace=False)\n  )\n  # decoder embedding\uff0c\u89e3\u7801\u7aef\u7684embedding\n  (dec_embedding): DataEmbedding(\n    (value_embedding): TokenEmbedding(\n      (tokenConv): Conv1d(7, 512, kernel_size=(3,), stride=(1,), padding=(1,), padding_mode=circular)\n    )\n    (position_embedding): PositionalEmbedding()\n    (temporal_embedding): TimeFeatureEmbedding(\n      (embed): Linear(in_features=4, out_features=512, bias=True)\n    )\n    (dropout): Dropout(p=0.05, inplace=False)\n  )\n  # encoder\u90e8\u5206\n  (encoder): Encoder(\n    (attn_layers): ModuleList(\n      # \u4e09\u4e2a\u5305\u88c5\u6ce8\u610f\u529b\u7684encoderlayer\uff0c\u5e26\u7740conv1/conv2\n      (0,1,2): EncoderLayer(\n        (attention): AttentionLayer(\n          (inner_attention): ProbAttention(\n            (dropout): Dropout(p=0.05, inplace=False)\n          )\n          (query_projection): Linear(in_features=512, out_features=512, bias=True)\n          (key_projection): Linear(in_features=512, out_features=512, bias=True)\n          (value_projection): Linear(in_features=512, out_features=512, bias=True)\n          (out_projection): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))\n        (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout): Dropout(p=0.05, inplace=False)\n      )\n    )\n    # 2\u4e2a\u5377\u79ef\u5c42\n    (conv_layers): ModuleList(\n      (0,1): ConvLayer(\n        (downConv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), padding_mode=circular)\n        (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (activation): ELU(alpha=1.0)\n        (maxPool): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n      )\n    )\n    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n  )\n  (decoder): Decoder(\n    (layers): ModuleList(\n      (0,1): DecoderLayer(\n        (self_attention): AttentionLayer(\n          (inner_attention): ProbAttention(\n            (dropout): Dropout(p=0.05, inplace=False)\n          )\n          (query_projection): Linear(in_features=512, out_features=512, bias=True)\n          (key_projection): Linear(in_features=512, out_features=512, bias=True)\n          (value_projection): Linear(in_features=512, out_features=512, bias=True)\n          (out_projection): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (cross_attention): AttentionLayer(\n          (inner_attention): FullAttention(\n            (dropout): Dropout(p=0.05, inplace=False)\n          )\n          (query_projection): Linear(in_features=512, out_features=512, bias=True)\n          (key_projection): Linear(in_features=512, out_features=512, bias=True)\n          (value_projection): Linear(in_features=512, out_features=512, bias=True)\n          (out_projection): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))\n        (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout): Dropout(p=0.05, inplace=False)\n      )\n    )\n    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n  )\n  (projection): Linear(in_features=512, out_features=7, bias=True)\n)\n</code></pre> Python<pre><code># enc_embedding\ndef forward(self, x, x_mark):\n    #x[32,96,7],x_mark[32,96,4]\n    x = self.value_embedding(x) + self.position_embedding(x) + self.temporal_embedding(x_mark)\n    \"\"\"\n    value_embedding x[32,96,512]\n    position_embedding x[1,96,512]\n    temporal_embedding x[32,96,512]\n    \"\"\"\n    return self.dropout(x)\n#TokenEmbedding\ndef forward(self, x):\n    x = self.tokenConv(x.permute(0, 2, 1)).transpose(1,2)\n    #x[32,96,512],permute\u7528\u4e8e\u6539\u53d8\u5f20\u91cf\u7ef4\u5ea6\u987a\u5e8f\uff0c\u4e0d\u6539\u53d8\u5185\u5bb9\u672c\u8eab\n    return x\n</code></pre> Python<pre><code># Informer\ndef forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec,\n                enc_self_mask=None, dec_self_mask=None, dec_enc_mask=None):\n        \"\"\"\n        x_enc[32,96,7],x_mark_enc[32,96,4]\n        x_dec[32,72,7],x_mark_dec[32,72,4]\n        \"\"\"\n        enc_out = self.enc_embedding(x_enc, x_mark_enc)\n        # enc_out[32,96,512]\n        enc_out, attns = self.encoder(enc_out, attn_mask=enc_self_mask)\n        # enc_out[32,24,512]\n\n        dec_out = self.dec_embedding(x_dec, x_mark_dec)\n        # dec_out[32,72,512]\n        dec_out = self.decoder(dec_out, enc_out, x_mask=dec_self_mask, cross_mask=dec_enc_mask)\n        # dec_out[32,72,512]\n        dec_out = self.projection(dec_out)\n        #dec_out[32,72,7]\n        # dec_out = self.end_conv1(dec_out)\n        # dec_out = self.end_conv2(dec_out.transpose(2,1)).transpose(1,2)\n        if self.output_attention:\n            return dec_out[:, -self.pred_len:, :], attns\n        else:\n            return dec_out[:, -self.pred_len:, :]  # [B, L, D]\n</code></pre>"},{"location":"DeepL/Paper/informer/#encoder","title":"Encoder","text":"<p>\u4e0a\u56fe\u5c55\u793a\u4e86 EncoderStack \u7684\u67b6\u6784\u56fe\u3002\u4e00\u4e2a EncoderStack \u91cc\u9762\u5305\u62ec\u82e5\u5e72\u4e2a\u7ea7\u8054\u7684 Encoder\uff0c\u800c\u4e00\u4e2a Encoder \u5185\u90e8\u53c8\u5305\u62ec\u4e86\u82e5\u5e72\u4e2a EncoderLayer \u548c ConvLayer\u3002\u5176\u4e2d\u5728 EncoderLayer \u5185\u90e8\u52a0\u5165\u4e86 ProbSparse Self-Attention\u3002</p> Python<pre><code># EncodeLayer\ndef forward(self, x, attn_mask=None):\n    # x [B, L, D]\n    # x = x + self.dropout(self.attention(\n    #     x, x, x,\n    #     attn_mask = attn_mask\n    # ))\n    new_x, attn = self.attention(x, x, x,\n        attn_mask = attn_mask\n    )\n    # new_x[32,96,512]\n    x = x + self.dropout(new_x)\n\n    y = x = self.norm1(x)\n    y = self.dropout(self.activation(self.conv1(y.transpose(-1,1))))\n    # conv1: Conv1d(2048,512,kernel_size=(1,),stridf=(1,))\n    y = self.dropout(self.conv2(y).transpose(-1,1))\n    # conv2: Conv1d(512,2048,kernel_size=(1,),stridf=(1,))\n    return self.norm2(x+y), attn\n</code></pre> Python<pre><code># Encoder\ndef forward(self, x, attn_mask=None):\n  # x [B, L, D]\n  attns = []\n  if self.conv_layers is not None:\n      for attn_layer, conv_layer in zip(self.attn_layers, self.conv_layers):\n          x, attn = attn_layer(x, attn_mask=attn_mask)\n          x = conv_layer(x)\n          attns.append(attn)\n        x, attn = self.attn_layers[-1](x, attn_mask=attn_mask)\n        attns.append(attn)\n  else:\n      for attn_layer in self.attn_layers:\n          x, attn = attn_layer(x, attn_mask=attn_mask)\n          attns.append(attn)\n\n  if self.norm is not None:\n    x = self.norm(x)\n\n  return x, attns\n</code></pre>"},{"location":"DeepL/Paper/informer/#decoder","title":"Decoder","text":"<p>Decoder \u7684 Embedding \u4e0e Encoder \u7684 Embedding \u64cd\u4f5c\u5b8c\u5168\u76f8\u540c\uff0c\u53ea\u662f\u8f93\u5165\u53d8\u4e3a <code>[32,72,7]</code></p> Python<pre><code># DecoderLayer\ndef forward(self, x, cross, x_mask=None, cross_mask=None):\n  #x[32,72,512], cross[32,24,512]\n  x = x + self.dropout(self.self_attention(\n      x, x, x,\n      attn_mask=x_mask\n  )[0])\n  x = self.norm1(x)\n\n  x = x + self.dropout(self.cross_attention(\n      x, cross, cross,\n      attn_mask=cross_mask\n  )[0])\n\n  y = x = self.norm2(x)\n  #y[32,2048,512]\n  y = self.dropout(self.activation(self.conv1(y.transpose(-1,1))))\n  # y[32,72,512]\n  y = self.dropout(self.conv2(y).transpose(-1,1))\n\n  return self.norm3(x+y)\n</code></pre> Python<pre><code># Decoder\ndef forward(self, x, cross, x_mask=None, cross_mask=None):\n  #x[32,72,512], cross[32,24,512]\n  for layer in self.layers:\n    x = layer(x, cross, x_mask=x_mask, cross_mask=cross_mask)\n\n  if self.norm is not None:\n    x = self.norm(x)\n\n  return x\n</code></pre>"},{"location":"DeepL/Paper/informer/#probsparse-self-attention","title":"ProbSparse Self-Attention","text":"<p>\u4f20\u7edf Self-Attention \u9700\u8981 \\(O(L_QL_K)\\) \u7684\u5185\u5b58\u4ee5\u53ca\u4e8c\u6b21\u70b9\u79ef\u8ba1\u7b97\uff0c\u662f\u5176\u4e3b\u8981\u7f3a\u70b9\u3002\u672c\u6587\u7814\u7a76\u53d1\u73b0\uff0c\u5e76\u4e0d\u662f\u6bcf\u4e2a Q \u4e0e K \u4e4b\u95f4\u90fd\u6709\u5f88\u9ad8\u7684\u76f8\u5173\u6027\uff08\u70b9\u79ef\uff09\uff0c\u6545\u53ea\u6709\u5c11\u6570\u70b9\u79ef\u5bf9\u4e3b\u8981\u6ce8\u610f\u529b\u8ba1\u7b97\u6709\u8d21\u732e\uff0c\u5176\u4f59\u53ef\u4ee5\u5ffd\u7565\u3002</p> <p>\u6539\u8fdb\u7b97\u6cd5\u5982\u4e0b\uff1a</p> <ul> <li>\u8f93\u5165\u5e8f\u5217\u957f\u5ea6\u4e3a 96\uff0c\u9996\u5148\u5728 K \u4e2d\u8fdb\u884c\u91c7\u6837\uff0c\u968f\u673a\u9009\u53d6 25 \u4e2a K\u3002</li> <li>\u8ba1\u7b97\u6bcf\u4e2a Q \u4e0e 25 \u4e2a K \u7684\u5185\u79ef\u3002</li> <li>\u5728\u6bcf\u4e2a Q \u7684 25 \u4e2a\u7ed3\u679c\u4e2d\uff0c\u9009\u62e9\u6700\u5927\u503c\u4e0e\u5747\u503c\u8ba1\u7b97\u5dee\u5f02\u3002</li> <li>\u5c06\u5dee\u5f02\u4ece\u5927\u5230\u5c0f\u6392\u5217\uff0c\u9009\u51fa\u5dee\u5f02\u524d 25 \u5927\u7684 Q\u3002</li> <li>\u5176\u4f59\u6dd8\u6c70\u7684 Q \u4f7f\u7528 V \u7684\u5e73\u5747\u5411\u91cf\u8fdb\u884c\u4ee3\u66ff\u3002</li> </ul>"},{"location":"DeepL/Paper/informer/#self-attention-distilling","title":"Self-Attention Distilling","text":"Python<pre><code># AttentionLayer\ndef forward(self, queries, keys, values, attn_mask):\n    # shape[queries=keys=values]:[32,96,512]\n    B, L, _ = queries.shape # L\uff0cS\u90fd\u4e3a96\n    _, S, _ = keys.shape\n    H = self.n_heads\n\n    # [32,96,512]-&gt;[32,96,8,64]\n    queries = self.query_projection(queries).view(B, L, H, -1)\n    keys = self.key_projection(keys).view(B, S, H, -1)\n    values = self.value_projection(values).view(B, S, H, -1)\n\n    out, attn = self.inner_attention(\n        queries,\n        keys,\n        values,\n        attn_mask\n    )\n    if self.mix:\n        out = out.transpose(2,1).contiguous()\n    out = out.view(B, L, -1)\n\n    return self.out_projection(out), attn\n</code></pre> Python<pre><code>def _prob_QK(self, Q, K, sample_k, n_top): # n_top: c*ln(L_q)\n    # Q [B, H, L, D][32,8,96,64]\n    B, H, L_K, E = K.shape #L_k:96,E:64,B:32,H:8\n    _, _, L_Q, _ = Q.shape #L_Q:96\n\n    # calculate the sampled Q_K\n    # K_expand[32,8,96,96,64]\n    K_expand = K.unsqueeze(-3).expand(B, H, L_Q, L_K, E)\n    \"\"\"\n    index_sample[96,25]\uff0ctorch.randint\u4ece[0,24]\u7684\u8303\u56f4\u968f\u673a\u751f\u6210\u6574\u6570\uff0c\u7528\u4e8e\n    \u6784\u5efa\u5f62\u72b6\u4e3a[96,25]\u7684\u5f20\u91cf\n    \"\"\"\n    index_sample = torch.randint(L_K, (L_Q, sample_k)) # real U = U_part(factor*ln(L_k))*L_q\n    \"\"\"\n    torch.arange(L_Q).unsqueeze(1)\u751f\u6210\u4e00\u4e2a\u5f62\u72b6\u4e3a[96,1]\u7684\u5f20\u91cf\uff0c\u5185\u5bb9\u4e3a[0,95]\u6784\u6210\u7684\u5e8f\u5217\uff0c\u52a0\u4e0aunsqueeze(1)\u540e\uff0c\u5176\u4ece\u5f62\u72b6[96]\u53d8\u4e3a[96,1];\n    index_sample\u662f\u5f62\u72b6\u4e3a[96,25]\u7684\u5f20\u91cf\uff0c\n    \"\"\"\n    K_sample = K_expand[:, :, torch.arange(L_Q).unsqueeze(1), index_sample, :]\n    Q_K_sample = torch.matmul(Q.unsqueeze(-2), K_sample.transpose(-2, -1)).squeeze(-2)\n\n    # find the Top_k query with sparisty measurement\n    M = Q_K_sample.max(-1)[0] - torch.div(Q_K_sample.sum(-1), L_K)\n    M_top = M.topk(n_top, sorted=False)[1]\n\n    # use the reduced Q to calculate Q_K\n    Q_reduce = Q[torch.arange(B)[:, None, None],torch.arange(H)[None, :, None],\n            M_top, :] # factor*ln(L_q)\n    Q_K = torch.matmul(Q_reduce, K.transpose(-2, -1)) # factor*ln(L_q)*L_k\n\n    return Q_K, M_top\n</code></pre>"},{"location":"DeepL/Paper/informer/#_2","title":"\u90e8\u5206\u6570\u636e\u96c6\u7ed3\u679c","text":"ETTh1 ETTh2 mse 0.428 0.248 mae 0.580 0.405"},{"location":"DeepL/Paper/pathformer/","title":"Pathformer","text":""},{"location":"DeepL/Paper/pathformer/#_1","title":"\u7814\u7a76\u95ee\u9898","text":"<p>\u81ea\u9002\u5e94\u591a\u5c3a\u5ea6 Transformer \u65f6\u95f4\u5e8f\u5217\u9884\u6d4b</p> <p>\u5982\u4f55\u6709\u6548\u5730\u6355\u6349\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u7684\u591a\u5c3a\u5ea6\u4f9d\u8d56\u5173\u7cfb\uff0c\u4ece\u800c\u63d0\u9ad8\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u5177\u4f53\u7684\u6311\u6218\u5305\u62ec\uff1a</p> <ol> <li>\u6355\u6349\u8de8\u5c3a\u5ea6\u7684\u65f6\u95f4\u4f9d\u8d56\u6027\uff1a\u4f20\u7edf\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\uff0c\u5982\u6807\u51c6\u7684Transformer\uff0c\u901a\u5e38\u5904\u7406\u56fa\u5b9a\u5c3a\u5ea6\u7684\u65f6\u95f4\u5e8f\u5217\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u4e0d\u540c\u65f6\u95f4\u7c92\u5ea6\u4e0a\u7684\u5efa\u6a21\u80fd\u529b\u3002\u65f6\u95f4\u5e8f\u5217\u4e2d\u53ef\u80fd\u5305\u542b\u77ed\u671f\u7684\u6ce2\u52a8\uff08\u5982\u5c0f\u65f6\u7ea7\u522b\uff09\u548c\u957f\u671f\u7684\u8d8b\u52bf\uff08\u5982\u5e74\u5ea6\u8d8b\u52bf\uff09\uff0c\u5982\u4f55\u540c\u65f6\u5efa\u6a21\u8fd9\u4e24\u79cd\u4f9d\u8d56\u5173\u7cfb\u662f\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u3002</li> <li>\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u591a\u5c3a\u5ea6\u5efa\u6a21\uff1a\u5982\u4f55\u8bbe\u8ba1\u4e00\u79cd\u65b9\u6cd5\u80fd\u591f\u5728\u4e0d\u540c\u7684\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u5bf9\u6570\u636e\u8fdb\u884c\u6709\u6548\u5efa\u6a21\uff0c\u5e76\u6355\u6349\u5168\u5c40\u4e0e\u5c40\u90e8\u7684\u65f6\u95f4\u4f9d\u8d56\u6027\uff0c\u5c24\u5176\u662f\u5728\u9762\u5bf9\u590d\u6742\u548c\u52a8\u6001\u53d8\u5316\u7684\u65f6\u5e8f\u6570\u636e\u65f6\uff0c\u5982\u4f55\u5904\u7406\u8f93\u5165\u6570\u636e\u7684\u591a\u53d8\u6027\u548c\u590d\u6742\u6027\u3002</li> <li>\u81ea\u9002\u5e94\u5efa\u6a21\uff1a\u5982\u4f55\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u6839\u636e\u8f93\u5165\u6570\u636e\u7684\u4e0d\u540c\u52a8\u6001\u53d8\u5316\u81ea\u9002\u5e94\u5730\u8c03\u6574\u591a\u5c3a\u5ea6\u5efa\u6a21\u8fc7\u7a0b\uff0c\u4ee5\u63d0\u9ad8\u5176\u9884\u6d4b\u80fd\u529b\u548c\u6cdb\u5316\u80fd\u529b\u3002</li> </ol>"},{"location":"DeepL/Paper/pathformer/#_2","title":"\u7814\u7a76\u73b0\u72b6","text":"<p>\u5728\u65f6\u5e8f\u9884\u6d4b\u4e2d\uff0c\u201c\u5c3a\u5ea6\u201d\uff08scale\uff09\u901a\u5e38\u6307\u7684\u662f\u65f6\u95f4\u5e8f\u5217\u4e2d\u6570\u636e\u7684\u4e0d\u540c\u65f6\u95f4\u7c92\u5ea6\u6216\u5206\u8fa8\u7387\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5c3a\u5ea6\u53ef\u4ee5\u4ece\u4ee5\u4e0b\u51e0\u4e2a\u65b9\u9762\u7406\u89e3\uff1a</p> <ol> <li>\u65f6\u95f4\u7c92\u5ea6\uff08Temporal Granularity\uff09\uff1a\u5c3a\u5ea6\u53ef\u4ee5\u6307\u65f6\u95f4\u5e8f\u5217\u4e2d\u6570\u636e\u7684\u65f6\u95f4\u95f4\u9694\u3002\u4f8b\u5982\uff0c\u5982\u679c\u4f60\u6709\u6309\u5c0f\u65f6\u3001\u6309\u5929\u6216\u6309\u5206\u949f\u8bb0\u5f55\u7684\u6570\u636e\uff0c\u90a3\u4e48\u8fd9\u4e9b\u4e0d\u540c\u7684\u65f6\u95f4\u95f4\u9694\u53ef\u4ee5\u89c6\u4e3a\u4e0d\u540c\u7684\u5c3a\u5ea6\u3002\u5728\u77ed\u671f\u9884\u6d4b\u4e2d\uff0c\u4f60\u53ef\u80fd\u5173\u6ce8\u5206\u949f\u7ea7\u522b\u7684\u6570\u636e\uff0c\u800c\u5728\u957f\u671f\u9884\u6d4b\u4e2d\uff0c\u4f60\u53ef\u80fd\u5173\u6ce8\u7684\u662f\u5929\u7ea7\u6216\u6708\u7ea7\u6570\u636e\u3002</li> <li>\u65f6\u95f4\u4f9d\u8d56\u6027\uff08Temporal Dependency\uff09\uff1a\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u67d0\u4e9b\u6a21\u5f0f\u53ef\u80fd\u5728\u8f83\u77ed\u65f6\u95f4\u5185\u53d1\u751f\u53d8\u5316\uff08\u5982\u5b63\u8282\u6027\u6ce2\u52a8\u6216\u65e5\u5e38\u53d8\u5316\uff09\uff0c\u800c\u5176\u4ed6\u6a21\u5f0f\u53ef\u80fd\u9700\u8981\u8f83\u957f\u65f6\u95f4\u6765\u663e\u73b0\uff08\u5982\u957f\u671f\u8d8b\u52bf\uff09\u3002\u56e0\u6b64\uff0c\u5c3a\u5ea6\u4e5f\u53ef\u4ee5\u6307\u7684\u662f\u6355\u6349\u4e0d\u540c\u65f6\u95f4\u8de8\u5ea6\u4e0a\u4f9d\u8d56\u5173\u7cfb\u7684\u80fd\u529b\u3002\u4f8b\u5982\uff0c\u77ed\u671f\u5c3a\u5ea6\u7528\u4e8e\u6355\u6349\u77ed\u671f\u4f9d\u8d56\u5173\u7cfb\uff08\u5982\u6bcf\u5929\u7684\u6ce2\u52a8\uff09\uff0c\u800c\u957f\u671f\u5c3a\u5ea6\u7528\u4e8e\u6355\u6349\u957f\u671f\u4f9d\u8d56\u5173\u7cfb\uff08\u5982\u6574\u4f53\u8d8b\u52bf\uff09\u3002</li> <li>\u591a\u5c3a\u5ea6\u5efa\u6a21\uff1a\u5728\u591a\u5c3a\u5ea6\u5efa\u6a21\u4e2d\uff0c\u6a21\u578b\u80fd\u591f\u540c\u65f6\u5904\u7406\u4e0d\u540c\u65f6\u95f4\u5206\u8fa8\u7387\u7684\u6570\u636e\uff0c\u4ece\u800c\u80fd\u591f\u5728\u4e0d\u540c\u7684\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u63d0\u53d6\u7279\u5f81\u3002\u8fd9\u79cd\u65b9\u6cd5\u80fd\u591f\u66f4\u5168\u9762\u5730\u6355\u6349\u65f6\u5e8f\u6570\u636e\u7684\u4e0d\u540c\u52a8\u6001\u7279\u6027\uff0c\u5982\u77ed\u671f\u53d8\u5316\u548c\u957f\u671f\u8d8b\u52bf\u3002\u4f8b\u5982\uff0cPathformer\u901a\u8fc7\u5c06\u65f6\u95f4\u5e8f\u5217\u5206\u4e3a\u4e0d\u540c\u7684\u65f6\u95f4\u5c3a\u5ea6\uff08\u5982\u5c0f\u65f6\u3001\u5929\u3001\u5468\u7b49\uff09\uff0c\u5e76\u4f7f\u7528\u4e0d\u540c\u5c3a\u5ea6\u7684\u6ce8\u610f\u529b\u673a\u5236\u6765\u5efa\u6a21\u5168\u5c40\u4e0e\u5c40\u90e8\u7684\u4f9d\u8d56\u5173\u7cfb\u3002</li> </ol> <p>\u5355\u5c3a\u5ea6\u5efa\u6a21\u6311\u6218\uff1a</p> <ul> <li>\u65e0\u6cd5\u6355\u6349\u591a\u5c3a\u5ea6\u7684\u7279\u5f81</li> <li>\u77ed\u671f\u4e0e\u957f\u671f\u4f9d\u8d56\u5173\u7cfb\u96be\u4ee5\u5e73\u8861</li> <li>\u6cdb\u5316\u80fd\u529b\u6709\u9650</li> </ul>"},{"location":"DeepL/Paper/pathformer/#_3","title":"\u521b\u65b0\u70b9","text":"<ul> <li>\u591a\u5c3a\u5ea6\u5efa\u6a21\uff0c\u6574\u5408\u4e86\u65f6\u95f4\u5206\u8fa8\u7387\u548c\u65f6\u95f4\u8ddd\u79bb\u4e24\u4e2a\u89c6\u89d2</li> <li>\u81ea\u9002\u5e94\u8def\u5f84\uff0c\u7531\u591a\u5c3a\u5ea6\u8def\u7531\u5668\u4e0e\u65f6\u95f4\u5206\u89e3\u534f\u540c\u5de5\u4f5c</li> <li>\u53cc\u91cd\u6ce8\u610f\u529b\u673a\u5236</li> </ul>"},{"location":"DeepL/Paper/pathformer/#_4","title":"\u6a21\u578b\u6846\u67b6","text":""},{"location":"DeepL/Paper/pathformer/#_5","title":"\u591a\u5c3a\u5ea6\u5212\u5206","text":"<p>\u4e3a\u4e86\u7b80\u5316\u7b26\u53f7\u8868\u793a\uff0c\u6211\u4eec\u4f7f\u7528\u5355\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u8fdb\u884c\u63cf\u8ff0\uff0c\u4e14\u8be5\u65b9\u6cd5\u53ef\u4ee5\u901a\u8fc7\u72ec\u7acb\u5904\u7406\u6bcf\u4e2a\u53d8\u91cf\uff0c\u8f7b\u677e\u6269\u5c55\u5230\u591a\u53d8\u91cf\u60c5\u51b5\u3002\u5728\u591a\u5c3a\u5ea6 Transformer \u6a21\u5757\u4e2d\uff0c\u6211\u4eec\u5b9a\u4e49\u4e00\u4e2a\u5305\u542b M \u4e2a\u8865\u4e01\u5927\u5c0f\u503c\u7684\u96c6\u5408 \\(S = \\{S_1, S_2, \\dots, S_M\\}\\)\uff0c\u5176\u4e2d\u6bcf\u4e2a\u8865\u4e01\u5927\u5c0f S \u5bf9\u5e94\u4e00\u4e2a\u8865\u4e01\u5212\u5206\u64cd\u4f5c\u3002\u5bf9\u4e8e\u8f93\u5165\u65f6\u95f4\u5e8f\u5217 \\(X \\in \\mathbb{R}^{H \\times d}\\)\uff0c\u5176\u4e2d H \u8868\u793a\u65f6\u95f4\u5e8f\u5217\u7684\u957f\u5ea6\uff0cd \u8868\u793a\u7279\u5f81\u7684\u7ef4\u5ea6\uff0c\u6bcf\u4e2a\u8865\u4e01\u5212\u5206\u64cd\u4f5c\u4ee5\u8865\u4e01\u5927\u5c0f S \u5c06 X \u5212\u5206\u4e3a P \u4e2a\u8865\u4e01\uff08\u5176\u4e2d \\(P=H/S\\)\uff09\uff0c\u5f62\u5f0f\u4e3a \\((X_1, X_2, \\dots, X_P)\\)\uff0c\u5176\u4e2d\u6bcf\u4e2a\u8865\u4e01 \\(X_i \\in \\mathbb{R}^{S \\times d}\\) \u5305\u542b S \u4e2a\u65f6\u95f4\u6b65\u3002\u96c6\u5408\u4e2d\u7684\u4e0d\u540c\u8865\u4e01\u5927\u5c0f\u5c06\u5bfc\u81f4\u4e0d\u540c\u5c3a\u5ea6\u7684\u5212\u5206\u8865\u4e01\uff0c\u4ece\u800c\u4e3a\u8f93\u5165\u5e8f\u5217\u63d0\u4f9b\u4e0d\u540c\u7684\u65f6\u95f4\u5206\u8fa8\u7387\u89c6\u89d2\u3002\u8fd9\u79cd\u591a\u5c3a\u5ea6\u5212\u5206\u4e0e\u4e0b\u9762\u63cf\u8ff0\u7684\u53cc\u91cd\u6ce8\u610f\u529b\u673a\u5236\u4e00\u8d77\u5de5\u4f5c\uff0c\u7528\u4e8e\u591a\u5c3a\u5ea6\u5efa\u6a21\u3002</p> <p></p>"},{"location":"DeepL/Paper/pathformer/#_6","title":"\u53cc\u91cd\u6ce8\u610f\u529b","text":"<p>\u53cc\u91cd\u6ce8\u610f\u529b\u673a\u5236\u5305\u62ec\u4e86\u4e24\u90e8\u5206\uff1a\u8865\u4e01\u5185\u6ce8\u610f\u529b\uff0c\u5373\u5728\u6bcf\u4e2a\u5212\u5206\u7684\u8865\u4e01\u5185\u8fdb\u884c\u7684\u6ce8\u610f\u529b\u64cd\u4f5c\uff1b\u8865\u4e01\u95f4\u6ce8\u610f\u529b\uff0c\u5373\u8de8\u4e0d\u540c\u8865\u4e01\u4e4b\u95f4\u7684\u6ce8\u610f\u529b\u64cd\u4f5c\u3002 </p> <p>\u878d\u5408\u5168\u5c40\u548c\u5c40\u90e8\u4fe1\u606f\uff1a\u4e3a\u4e86\u878d\u5408\u901a\u8fc7\u53cc\u91cd\u6ce8\u610f\u529b\u673a\u5236\u6355\u6349\u7684\u5168\u5c40\u548c\u5c40\u90e8\u4fe1\u606f\uff0c\u6211\u4eec\u5bf9\u8865\u4e01\u5185\u6ce8\u610f\u529b\u7684\u8f93\u51fa \\(\\text{Attn}_{intra}\\) \u8fdb\u884c\u91cd\u6392\uff0c\u5f97\u5230 \\(\\text{Attn}_{intra} \\in \\mathbb{R}^{P \\times S \\times d_m}\\)\uff0c\u5e76\u901a\u8fc7\u7ebf\u6027\u53d8\u6362\u5c06\u8865\u4e01\u5927\u5c0f\u4ece 1 \u53d8\u4e3a S\uff0c\u4ee5\u5408\u5e76\u6bcf\u4e2a\u8865\u4e01\u4e2d\u7684\u65f6\u95f4\u6b65\u3002\u6700\u7ec8\uff0c\u5c06\u5176\u4e0e\u8865\u4e01\u95f4\u6ce8\u610f\u529b \\(\\text{Attn}_{inter}\\) \u76f8\u52a0\uff0c\u5f97\u5230\u53cc\u91cd\u6ce8\u610f\u529b\u7684\u6700\u7ec8\u8f93\u51fa\uff1a\\(\\text{Attn} = \\text{Attn}_{intra} + \\text{Attn}_{inter}\\)</p>"},{"location":"DeepL/Paper/pathformer/#_7","title":"\u591a\u5c3a\u5ea6\u8def\u7531","text":"<p>\u591a\u5c3a\u5ea6\u8def\u7531\u5668\uff08Multi-Scale Router\uff09 \u662f\u591a\u5c3a\u5ea6 Transformer \u4e2d\u7684\u4e00\u90e8\u5206\uff0c\u7528\u4e8e\u6570\u636e\u81ea\u9002\u5e94\u8def\u7531\uff0c\u9009\u62e9\u6700\u4f18\u7684\u8865\u4e01\u5927\u5c0f\uff0c\u4ece\u800c\u63a7\u5236\u591a\u5c3a\u5ea6\u5efa\u6a21\u7684\u8fc7\u7a0b\u3002\u8fd9\u4e2a\u8def\u7531\u5668\u7ed3\u5408\u4e86\u65f6\u95f4\u5e8f\u5217\u7684\u5468\u671f\u6027\u548c\u8d8b\u52bf\u6027\u5206\u89e3\uff0c\u4ee5\u63d0\u53d6\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u5468\u671f\u6027\u548c\u8d8b\u52bf\u6a21\u5f0f\uff0c\u5177\u4f53\u8fc7\u7a0b\u5982\u4e0b\uff1a</p> <ol> <li> <p>\u5b63\u8282\u6027\u5206\u89e3\uff08Seasonality Decomposition\uff09\uff1a</p> <ul> <li>\u5085\u91cc\u53f6\u53d8\u6362\uff1a\u9996\u5148\u5c06\u65f6\u95f4\u5e8f\u5217\u4ece\u65f6\u57df\u8f6c\u6362\u5230\u9891\u57df\uff0c\u4f7f\u7528 Discern Fourier Transform\uff08DFT\uff09 \u63d0\u53d6\u5468\u671f\u6027\u6a21\u5f0f\uff1a \\(X_{sea} = \\text{IDFT}(\\{f_1, \\dots, f_{K_f}\\}, A, \\Phi)\\) \u5176\u4e2d\uff0c\\(\\Phi\\) \u548c A \u662f\u5085\u91cc\u53f6\u53d8\u6362\u540e\u7684\u76f8\u4f4d\u548c\u5e45\u5ea6\uff0c \\({f_1, \\dots, f_{K_f}}\\) \u662f\u5e45\u5ea6\u6700\u5927 \\(K_f\\) \u4e2a\u9891\u7387\u6210\u5206\u3002\u901a\u8fc7\u9006\u5085\u91cc\u53f6\u53d8\u6362\u5f97\u5230\u5468\u671f\u6027\u6a21\u5f0f \\(X_{sea}\\)\u3002</li> </ul> </li> <li> <p>\u8d8b\u52bf\u6027\u5206\u89e3\uff08Trend Decomposition\uff09\uff1a</p> <ul> <li>\u5728\u5b63\u8282\u6027\u6a21\u5f0f\u63d0\u53d6\u540e\uff0c\u901a\u8fc7\u8ba1\u7b97\u5269\u4f59\u90e8\u5206 \\(X_{rem} = X - X_{sea}\\)\uff0c\u4f7f\u7528\u4e0d\u540c\u7684\u5e73\u5747\u6c60\u5316\u6838\u6765\u63d0\u53d6\u8d8b\u52bf\u6a21\u5f0f\uff1a \\(X_{trend} = \\text{Softmax}(L(X_{rem})) \\cdot \\left(\\text{AvgPool}(X_{rem})_{\\text{kernel1}}, \\dots, \\text{AvgPool}(X_{rem})_{\\text{kernelN}}\\right)\\) \u5176\u4e2d\uff0c<code>AvgPool</code> \u662f\u6c60\u5316\u51fd\u6570\uff0cSoftmax \u5904\u7406\u6c60\u5316\u7ed3\u679c\u7684\u6743\u91cd\u3002</li> </ul> </li> <li> <p>\u878d\u5408\u5b63\u8282\u6027\u4e0e\u8d8b\u52bf\u6a21\u5f0f\uff1a</p> <ul> <li>\u5c06\u5b63\u8282\u6027\u6a21\u5f0f\u548c\u8d8b\u52bf\u6a21\u5f0f\u4e0e\u539f\u59cb\u8f93\u5165 X \u76f8\u52a0\uff0c\u8fdb\u884c\u7ebf\u6027\u6620\u5c04 \\(\\text{Linear}(\u00b7)\\)\uff0c\u5f97\u5230\u878d\u5408\u540e\u7684\u8868\u793a \\(X_{trans} \\in \\mathbb{R}^d\\)</li> </ul> </li> <li> <p>\u8def\u7531\u51fd\u6570\uff1a</p> <ul> <li>\u57fa\u4e8e\u65f6\u95f4\u5206\u89e3\u7ed3\u679c \\(X_{trans}\\)\uff0c\u8def\u7531\u5668\u4f7f\u7528\u8def\u7531\u51fd\u6570\u6765\u751f\u6210\u8def\u5f84\u6743\u91cd\uff0c\u51b3\u5b9a\u5f53\u524d\u6570\u636e\u9009\u62e9\u7684\u8865\u4e01\u5927\u5c0f\uff1a \\(R(X_{trans}) = \\text{Softmax}(X_{trans} W_r + \\epsilon \\cdot \\text{Softplus}(X_{trans} W_{noise})), \\quad \\epsilon \\sim N(0, 1)\\) \u5176\u4e2d\uff0c\\(W_r\\) \u548c \\(W_{noise}\\) \u662f\u53ef\u5b66\u4e60\u7684\u53c2\u6570\uff0c\\(\\epsilon\\) \u662f\u566a\u58f0\u9879\uff0c\u7528\u6765\u5f15\u5165\u968f\u673a\u6027\u4ee5\u907f\u514d\u603b\u662f\u9009\u62e9\u76f8\u540c\u7684\u8865\u4e01\u5927\u5c0f\u3002</li> </ul> </li> <li> <p>\u7a00\u758f\u6027\u548c\u9009\u62e9\u5173\u952e\u5c3a\u5ea6\uff1a</p> <ul> <li>\u4e3a\u4e86\u9f13\u52b1\u9009\u62e9\u5173\u952e\u5c3a\u5ea6\u5e76\u5f15\u5165\u7a00\u758f\u6027\uff0c\u8def\u7531\u51fd\u6570\u4f7f\u7528 top-K \u9009\u62e9 \u6765\u4fdd\u7559\u6700\u91cd\u8981\u7684 K \u4e2a\u8def\u5f84\u6743\u91cd\uff0c\u5e76\u5c06\u5176\u4ed6\u8def\u5f84\u6743\u91cd\u8bbe\u4e3a 0\uff0c\u6700\u7ec8\u5f97\u5230\u7684\u8def\u5f84\u6743\u91cd\u8868\u793a\u4e3a \\(\\bar{R}(X_{trans})\\)\u3002</li> </ul> </li> </ol>"},{"location":"DeepL/Paper/pathformer/#_8","title":"\u591a\u5c3a\u5ea6\u805a\u5408","text":"<p>\u591a\u5c3a\u5ea6\u805a\u5408\u5668\uff08Multi-Scale Aggregator\uff09 \u662f\u591a\u5c3a\u5ea6 Transformer \u7684\u4e00\u90e8\u5206\uff0c\u7528\u4e8e\u5c06\u4e0d\u540c\u5c3a\u5ea6\u7684\u8f93\u51fa\u7ed3\u5408\u8d77\u6765\uff0c\u751f\u6210\u6700\u7ec8\u7684\u8f93\u51fa\u3002\u8be5\u6a21\u5757\u901a\u8fc7\u5bf9\u6bcf\u4e2a\u8865\u4e01\u5927\u5c0f\u7684\u8def\u5f84\u6743\u91cd\u8fdb\u884c\u52a0\u6743\u805a\u5408\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u591a\u5c3a\u5ea6\u8f93\u51fa\u7684\u7efc\u5408\u3002\u5177\u4f53\u8fc7\u7a0b\u5982\u4e0b\uff1a</p> <ol> <li>\u8def\u5f84\u6743\u91cd \\(\\bar{R}(X_{trans})\\)\uff1a<ul> <li>\u6bcf\u4e2a\u8def\u5f84\u6743\u91cd \\(\\bar{R}(X_{trans})_i &gt; 0\\) \u8868\u793a\u8be5\u8865\u4e01\u5927\u5c0f \\(S_i\\) \u5e94\u8be5\u53c2\u4e0e\u5904\u7406\uff0c\u8fdb\u884c\u8865\u4e01\u5212\u5206\u548c\u53cc\u91cd\u6ce8\u610f\u529b\uff1b\u5982\u679c \\(\\bar{R}(X_{trans})_i = 0\\)\uff0c\u5219\u8868\u793a\u5ffd\u7565\u8be5\u8865\u4e01\u5927\u5c0f\u3002</li> </ul> </li> <li>\u8f93\u51fa\u4e0e\u53d8\u6362\uff1a<ul> <li>\u5bf9\u4e8e\u6bcf\u4e2a\u8865\u4e01\u5927\u5c0f \\(S_i\\)\uff0c\u8f93\u51fa \\(X_i^{out}\\) \u662f\u591a\u5c3a\u5ea6 Transformer \u7684\u7ed3\u679c\u3002\u7531\u4e8e\u4e0d\u540c\u8865\u4e01\u5927\u5c0f\u4ea7\u751f\u7684\u65f6\u95f4\u7ef4\u5ea6\u53ef\u80fd\u4e0d\u540c\uff0c\u805a\u5408\u5668\u9996\u5148\u5e94\u7528\u4e00\u4e2a\u53d8\u6362\u51fd\u6570 \\(T_i(\\cdot)\\) \u6765\u5bf9\u9f50\u4e0d\u540c\u5c3a\u5ea6\u7684\u65f6\u95f4\u7ef4\u5ea6\u3002</li> </ul> </li> <li>\u52a0\u6743\u805a\u5408\uff1a<ul> <li>\u805a\u5408\u5668\u6839\u636e\u8def\u5f84\u6743\u91cd\u5bf9\u591a\u5c3a\u5ea6\u8f93\u51fa\u8fdb\u884c\u52a0\u6743\u805a\u5408\uff0c\u4ee5\u5f97\u5230\u6700\u7ec8\u7684\u8f93\u51fa \\(X_{out}\\)\u3002\u805a\u5408\u516c\u5f0f\u4e3a\uff1a\\(X_{out} = \\sum_{i=1}^{M} I(\\bar{R}(X_{trans})_i &gt; 0) \\, \\bar{R}(X_{trans})_i \\, T_i(X_i^{out})\\) \u5176\u4e2d\uff0c\\(I(\\bar{R}(X_{trans})_i &gt; 0)\\) \u662f\u6307\u793a\u51fd\u6570\uff0c\u5f53 \\(\\bar{R}(X_{trans})_i &gt; 0\\) \u65f6\u8f93\u51fa 1\uff0c\u5426\u5219\u8f93\u51fa 0\u3002\u8fd9\u610f\u5473\u7740\u5728\u52a0\u6743\u805a\u5408\u65f6\uff0c\u53ea\u8003\u8651\u5177\u6709\u6b63\u8def\u5f84\u6743\u91cd\u7684\u8865\u4e01\u5927\u5c0f\u53ca\u5176\u5bf9\u5e94\u7684 Transformer \u8f93\u51fa\u3002</li> </ul> </li> </ol>"},{"location":"DeepL/Paper/pathformer/#_9","title":"\u5b9e\u9a8c\u7ed3\u679c","text":""},{"location":"DeepL/Pytorch/time/","title":"CNN\u3001LSTM\u4e0eGRU","text":""},{"location":"DeepL/Pytorch/time/#conv1d","title":"Conv1D","text":"<p>\u5728\u65f6\u95f4\u5e8f\u5217\u4e2d\uff0cConv1D \u901a\u8fc7\u6ed1\u52a8\u4e00\u4e2a\u5377\u79ef\u6838\u6765\u63d0\u53d6\u5c40\u90e8\u7279\u5f81\uff0c\u6bcf\u6b21\u6ed1\u52a8\u65f6\u4e0e\u8f93\u5165\u6570\u636e\u7684\u4e00\u4e2a\u5c0f\u5757\u8fdb\u884c\u5377\u79ef\u8fd0\u7b97\u3002\u5377\u79ef\u7684\u76ee\u7684\u662f\u63d0\u53d6\u6570\u636e\u4e2d\u7684\u5c40\u90e8\u6a21\u5f0f\uff0c\u6bd4\u5982\u8d8b\u52bf\u3001\u5468\u671f\u6027\u53d8\u5316\u7b49\uff0c\u5e38\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u3001\u5f02\u5e38\u68c0\u6d4b\u7b49\u4efb\u52a1\u3002</p> <p>\u5728 PyTorch \u4e2d\uff0c<code>Conv1d</code> \u7684\u8f93\u5165\u548c\u8f93\u51fa\u901a\u5e38\u662f\u4e00\u4e2a\u4e09\u7ef4\u5f20\u91cf\uff0c\u5f62\u72b6\u4e3a\uff1a\\((batch\\_size, channels, length)\\)</p> <p>\u5176\u4e2d\uff1a</p> <ul> <li>\\(batch\\_size\\)\uff1a\u6bcf\u6279\u6570\u636e\u7684\u6837\u672c\u6570\u91cf\u3002</li> <li>\\(channels\\)\uff1a\u6bcf\u4e2a\u6837\u672c\u7684\u901a\u9053\u6570\uff0c\u5bf9\u4e8e\u4e00\u7ef4\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u901a\u5e38\u662f 1\u3002\u5982\u679c\u662f\u591a\u901a\u9053\u6570\u636e\uff08\u4f8b\u5982\u591a\u4e2a\u4f20\u611f\u5668\u7684\u65f6\u95f4\u5e8f\u5217\uff09\uff0c\u5219\u53ef\u4ee5\u6709\u591a\u4e2a\u901a\u9053\u3002</li> <li>\\(length\\)\uff1a\u65f6\u95f4\u5e8f\u5217\u7684\u957f\u5ea6\uff0c\u5373\u6bcf\u4e2a\u901a\u9053\u4e0a\u7684\u6570\u636e\u70b9\u6570\u3002</li> </ul> <p>\u5377\u79ef\u524d\u540e\u5e8f\u5217\u7684\u957f\u5ea6\u53d8\u5316\u4e3a\uff1a\\(L_{out} = \\frac{L_{in} - K + 2P}{S} + 1\\)</p> <p>\u5728\u5904\u7406\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u65f6\uff0c\u6c60\u5316\u64cd\u4f5c\uff08\u5982 Max Pooling \u548c Average Pooling\uff09\u4e5f\u53ef\u4ee5\u7528\u6765\u964d\u4f4e\u65f6\u95f4\u5e8f\u5217\u7684\u7ef4\u5ea6\uff0c\u4ece\u800c\u51cf\u5c11\u8ba1\u7b97\u91cf\u5e76\u589e\u5f3a\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002\u5bf9\u4e8e\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u6c60\u5316\u64cd\u4f5c\u901a\u5e38\u662f\u5bf9\u65f6\u95f4\u6b65\uff08\u5373\u5e8f\u5217\u7684\u957f\u5ea6\uff09\u8fdb\u884c\u4e0b\u91c7\u6837\uff0c\u800c\u4e0d\u662f\u5bf9\u7a7a\u95f4\u7ef4\u5ea6\u8fdb\u884c\u6c60\u5316\u3002\u6c60\u5316\u8f93\u5165\u8f93\u51fa\u5f62\u72b6\u548c\u5e8f\u5217\u957f\u5ea6\u53d8\u5316\u90fd\u4e0e\u5377\u79ef\u76f8\u540c\u3002</p>"},{"location":"DeepL/Pytorch/time/#_1","title":"\u6570\u636e\u5904\u7406","text":"<p>\u91c7\u7528\u65f6\u95f4\u7a97\u53e3\u5c06\u539f\u59cb\u65f6\u95f4\u5e8f\u5217\u5212\u5206\u4e3a\u82e5\u5e72\u56fa\u5b9a\u957f\u5ea6\u7684\u65f6\u95f4\u7247\u6bb5\uff0c\u6bcf\u4e2a\u65f6\u95f4\u7247\u6bb5\u7528\u4e8e\u9884\u6d4b\u672a\u6765\u7684\u503c\u3002</p> <p>\u5047\u8bbe\u539f\u59cb\u5e8f\u5217\u4e3a\uff1a\\(x_1,x_2,x_3,...,x_n\\)\uff0c\u7a97\u53e3\u5927\u5c0f\u4e3a 3\uff0c\u6b65\u957f\u4e3a 1\uff0c\u7a97\u53e3\u751f\u6210\u7684\u5b50\u5e8f\u5217\uff1a</p> <ul> <li>\u8f93\u5165\uff1a\\([x_1,x_2,x_3],[x_2,x_3,x_4],...\\)</li> <li>\u6807\u7b7e\uff1a\\([x_4],[x_5],...\\)</li> </ul> <p>\u65f6\u95f4\u5927\u5c0f\u5305\u542b\u8fc7\u53bb 12 \u4e2a\u65f6\u95f4\u6b65\uff0c\u6b65\u957f\u4e3a 1.</p> Python<pre><code>def create_dataset(data, time_step=12):\n    X, y = [], []\n    for i in range(len(data) - time_step):\n        X.append(data.iloc[i:i + time_step].values)\n        y.append(data.iloc[i + time_step, -1])\n    return np.array(X), np.array(y)\n\n\nX, y = create_dataset(data, 12)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n</code></pre> <p>\u5212\u5206\u6570\u636e\u540e\uff0c\u8fdb\u884c\u5f52\u4e00\u5316\u3002</p> <p><code>StandardScaler</code> \u53ea\u80fd\u5bf9\u4e8c\u7ef4\u6570\u7ec4\u7684\u6bcf\u5217\u8fdb\u884c\u5f52\u4e00\u5316\uff0c<code>X_train</code>(16,12,7)-&gt;(16*12,7)-&gt;(16,12,7)\u3002</p> Python<pre><code>class TimeSeriesDataset(Dataset):\n    def __init__(self, data, time_step=12, is_train=True, scaler_X=None, scaler_y=None):\n        \"\"\"\n        data: \u8f93\u5165\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e (DataFrame)\n        time_step: \u65f6\u95f4\u6b65\u957f\n        is_train: \u662f\u5426\u4e3a\u8bad\u7ec3\u96c6\n        scaler_X: \u7279\u5f81\u6807\u51c6\u5316\u7684Scaler\n        scaler_y: \u76ee\u6807\u6807\u51c6\u5316\u7684Scaler\n        \"\"\"\n        self.time_step = time_step\n        self.is_train = is_train\n\n        # \u521b\u5efa\u6837\u672c\u548c\u76ee\u6807\n        X, y = self.create_dataset(data)\n\n        # \u6807\u51c6\u5316\n        self.scaler_X = scaler_X or StandardScaler()\n        self.scaler_y = scaler_y or StandardScaler()\n\n        if self.is_train:\n            X = self.scaler_X.fit_transform(X.reshape(-1, X.shape[2])).reshape(X.shape)\n            y = self.scaler_y.fit_transform(y.reshape(-1, 1))\n        else:\n            X = self.scaler_X.transform(X.reshape(-1, X.shape[2])).reshape(X.shape)\n            y = self.scaler_y.transform(y.reshape(-1, 1))\n\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.float32)\n\n    def create_dataset(self, data):\n        X, y = [], []\n        for i in range(len(data) - self.time_step):\n            X.append(data.iloc[i:i + self.time_step].values)\n            y.append(data.iloc[i + self.time_step, -1])\n        return np.array(X), np.array(y)\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\n    def get_scalers(self):\n        \"\"\"\u8fd4\u56de\u7279\u5f81\u548c\u76ee\u6807\u7684\u6807\u51c6\u5316\u5668\"\"\"\n        return self.scaler_X, self.scaler_y\n</code></pre>"},{"location":"DeepL/Pytorch/time/#_2","title":"\u6a21\u578b\u6784\u5efa","text":"<p><code>Conv1d</code> \u63a5\u53d7\u8f93\u5165\u5f62\u72b6\u4e3a \\((batch\\_size, channels, length)\\)\uff0c\u6545\u9700\u8981\u5148\u8c03\u6574\u5f20\u91cf\u5f62\u72b6 <code>x = x.permute(0, 2, 1)</code>\u3002</p> Python<pre><code>class CNN_LSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, dropout_rate=0.5):\n        super(CNN_LSTM, self).__init__()\n        self.conv1 = nn.Conv1d(input_size, 16, kernel_size=1)\n        # self.conv2 = nn.Conv1d(16, input_size, kernel_size=1)\n        self.relu = nn.ReLU()\n        self.lstm = nn.LSTM(16, hidden_size, batch_first=True)\n        self.dropout = nn.Dropout(dropout_rate)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        x = x.permute(0, 2, 1)\n        x = self.relu(self.conv1(x))\n        x = self.dropout(x)\n        # x = self.relu(self.conv2(x))\n        # x = self.dropout(x)\n        x = x.permute(0, 2, 1)\n        _, (hn, _) = self.lstm(x)\n        return self.fc(hn[-1])\n        \"\"\"\n        output,_ = self.lstm(x)\n        last_output = output[:,-1,:]\n        return self.fc(last_output)\n        \"\"\"\n\ndevice = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# \u8bad\u7ec3\nmodel = CNN_LSTM(input_size=7, hidden_size=2, output_size=1, dropout_rate=0.5).to(device)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n</code></pre>"},{"location":"DeepL/Pytorch/time/#_3","title":"\u8bad\u7ec3\u4e0e\u6d4b\u8bd5","text":"Python<pre><code>avg_train_loss = 0.0\nfor epoch in range(100):\n    for X_batch, y_batch in train_loader:\n        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n        optimizer.zero_grad()\n        output = model(X_batch)\n        loss = criterion(output, y_batch)\n        avg_train_loss += loss.item() * X_batch.size(0)\n        loss.backward()\n        optimizer.step()\n    avg_train_loss /= len(train_loader)\n    if epoch % 20 == 19:\n        print(f\"Epoch {epoch + 1}/100, Average Loss: {avg_train_loss:.4f}\")\n\ntorch.save(model.state_dict(),'cnn_lstm.pth')\nprint(\"Model saved successfully.\")\n\nmodel = CNN_LSTM(input_size=7, hidden_size=1, output_size=1, dropout_rate=0.5).to(device)\nmodel.load_state_dict(torch.load('cnn_lstm.pth'))\nmodel.eval()\ny_pred = []\ny_true = []\ntest_loss = 0.0\nwith torch.no_grad():\n    for X_batch, y_batch in test_loader:\n        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n        output = model(X_batch)\n        loss = criterion(output, y_batch)\n        test_loss += loss.item() * X_batch.size(0)\n        y_pred.append(output.cpu().numpy())\n        y_true.append(y_batch.cpu().numpy())\ntest_loss /= len(test_loader)\nprint(test_loss)\n\n# np.concatenate\u5c06\u6240\u6709\u6279\u6b21\u6cbf\u7740axis = 0(\u884c\u65b9\u5411)\u62fc\u63a5\u8d77\u6765\uff0c\u5f62\u6210\u5b8c\u6574\u7684\u4e8c\u7ef4\u6570\u7ec4\uff0c\u62fc\u63a5\u540e\u5f62\u72b6\u4e3a(num_samples, output_size)\ny_pred = np.concatenate(y_pred, axis=0)\ny_true = np.concatenate(y_true, axis=0)\n\ny_pred = scaler_y.inverse_transform(y_pred)\ny_true = scaler_y.inverse_transform(y_true)\n\nmse = mean_squared_error(y_true, y_pred)\nmae = mean_absolute_error(y_true, y_pred)\n\nprint(f\"Test MSE: {mse:.4f}\")\nprint(f\"Test MAE: {mae:.4f}\")\n\nplt.figure(figsize=(12, 6))\nplt.plot(y_true, label='True Value', color='blue')\nplt.plot(y_pred, label='Predicted Value', color='red', linestyle='--')\nplt.title('True vs Predicted')\nplt.xlabel('Time (Samples)')\nplt.ylabel('Value')\nplt.legend()\nplt.savefig('ETTh1.jpg')\n</code></pre>"},{"location":"DeepL/Pytorch/time/#_4","title":"\u9884\u6d4b\u7ed3\u679c","text":"<p>CNN+LSTM\uff1a</p> <p>GRU:</p>"},{"location":"DeepL/Pytorch/time/#_5","title":"\u4f7f\u7528\u65f6\u95f4\u7a97\u53e3\u8fdb\u884c\u591a\u6b65\u9884\u6d4b","text":"<p>\u6bcf\u6b21\u9884\u6d4b\u4e00\u4e2a\u65f6\u95f4\u6b65\uff0c\u5e76\u5c06\u7ed3\u679c\u4f5c\u7528\u4e8e\u4e0b\u4e00\u6b21\u3002</p> Python<pre><code>def predict_future(model, last_data, future_steps, scaler_y, device):\n    model.eval()\n    predictions = []\n    current_input = torch.tensor(last_data, dtype=torch.float32).unsqueeze(0).to(\n        device)  # shape (1, time_step, features)\n\n    with torch.no_grad():\n        for _ in range(future_steps):\n            output = model(current_input)\n            predicted_value = output.cpu().numpy()  # shape (1, 1)\n            predictions.append(predicted_value)\n\n            predicted_tensor = torch.tensor(predicted_value, dtype=torch.float32).unsqueeze(0).repeat(1, 1,current_input.size(-1)).to(device)\n            current_input = torch.cat((current_input[:, 1:, :], predicted_tensor), dim=1)\n\n    predictions = np.array(predictions).squeeze()  # \u53bb\u6389\u591a\u4f59\u7684\u7ef4\u5ea6\n    return scaler_y.inverse_transform(predictions)  # \u8f6c\u6362\u56de\u539f\u59cb\u7684\u5c3a\u5ea6\n\n\nfuture_steps = 10\nlast_test_data = X_test_scaled[-1]  # \u6700\u540e\u4e00\u6761\u6d4b\u8bd5\u6570\u636e\u4f5c\u4e3a\u8d77\u70b9\npredicted_future = predict_future(model, last_test_data, future_steps, scaler_y, device)\n</code></pre>"},{"location":"DeepL/Pytorch/time/#cnn_lstmpy","title":"CNN_LSTM.py","text":"Python<pre><code>import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\n# def extract_time_features(df):\n#     df['date'] = pd.to_datetime(df['date'])\n#     df['year'] = df['date'].dt.year\n#     df['month'] = df['date'].dt.month\n#     df['day'] = df['date'].dt.day\n#     df['weekday'] = df['date'].dt.weekday\n#\n#     df = df.drop(columns=['date'])\n#     return df\n\n\n# \u6570\u636e\u52a0\u8f7d\u4e0e\u9884\u5904\u7406\ndata = pd.read_csv(\"ETTh1.csv\").drop(columns=['date'])\n\n\n# data = extract_time_features(data)\n\n# print(data.head())\n\n\nclass TimeSeriesDataset(Dataset):\n    def __init__(self, data, time_step=12, is_train=True, scaler_X=None, scaler_y=None):\n        \"\"\"\n        data: \u8f93\u5165\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e (DataFrame)\n        time_step: \u65f6\u95f4\u6b65\u957f\n        is_train: \u662f\u5426\u4e3a\u8bad\u7ec3\u96c6\n        scaler_X: \u7279\u5f81\u6807\u51c6\u5316\u7684Scaler\n        scaler_y: \u76ee\u6807\u6807\u51c6\u5316\u7684Scaler\n        \"\"\"\n        self.time_step = time_step\n        self.is_train = is_train\n\n        # \u521b\u5efa\u6837\u672c\u548c\u76ee\u6807\n        X, y = self.create_dataset(data)\n\n        # \u6807\u51c6\u5316\n        self.scaler_X = scaler_X or StandardScaler()\n        self.scaler_y = scaler_y or StandardScaler()\n\n        if self.is_train:\n            X = self.scaler_X.fit_transform(X.reshape(-1, X.shape[2])).reshape(X.shape)\n            y = self.scaler_y.fit_transform(y.reshape(-1, 1))\n        else:\n            X = self.scaler_X.transform(X.reshape(-1, X.shape[2])).reshape(X.shape)\n            y = self.scaler_y.transform(y.reshape(-1, 1))\n\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.float32)\n\n    def create_dataset(self, data):\n        X, y = [], []\n        for i in range(len(data) - self.time_step):\n            X.append(data.iloc[i:i + self.time_step].values)\n            y.append(data.iloc[i + self.time_step, -1])\n        return np.array(X), np.array(y)\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\n    def get_scalers(self):\n        \"\"\"\u8fd4\u56de\u7279\u5f81\u548c\u76ee\u6807\u7684\u6807\u51c6\u5316\u5668\"\"\"\n        return self.scaler_X, self.scaler_y\n\ntime_step = 12\n\n# \u6570\u636e\u96c6\u5212\u5206\ntrain_data, test_data = train_test_split(data, test_size=0.2, shuffle=False)\n\n# \u521d\u59cb\u5316\u6570\u636e\u96c6\ntrain_dataset = TimeSeriesDataset(train_data, time_step, is_train=True)\ntest_dataset = TimeSeriesDataset(test_data, time_step, is_train=False, scaler_X=train_dataset.scaler_X,\n                                 scaler_y=train_dataset.scaler_y)\n\nscaler_X, scaler_y = train_dataset.get_scalers()\n\n# \u6570\u636e\u52a0\u8f7d\u5668\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n\n# \u6a21\u578b\u5b9a\u4e49\nclass CNN_LSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, dropout_rate=0.5):\n        super(CNN_LSTM, self).__init__()\n        self.conv1 = nn.Conv1d(input_size, 16, kernel_size=1)\n        # self.conv2 = nn.Conv1d(16, input_size, kernel_size=1)\n        self.relu = nn.ReLU()\n        self.lstm = nn.LSTM(16, hidden_size, batch_first=True)\n        self.dropout = nn.Dropout(dropout_rate)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        x = x.permute(0, 2, 1)\n        x = self.relu(self.conv1(x))\n        x = self.dropout(x)\n        # x = self.relu(self.conv2(x))\n        # x = self.dropout(x)\n        x = x.permute(0, 2, 1)\n        _, (hn, _) = self.lstm(x)\n        return self.fc(hn[-1])\n\n\ndevice = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# \u8bad\u7ec3\nmodel = CNN_LSTM(input_size=7, hidden_size=1, output_size=1, dropout_rate=0.5).to(device)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n\navg_train_loss = 0.0\nfor epoch in range(100):\n    for X_batch, y_batch in train_loader:\n        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n        optimizer.zero_grad()\n        output = model(X_batch)\n        loss = criterion(output, y_batch)\n        avg_train_loss += loss.item() * X_batch.size(0)\n        loss.backward()\n        optimizer.step()\n    avg_train_loss /= len(train_loader)\n    if epoch % 20 == 19:\n        print(f\"Epoch {epoch + 1}/100, Average Loss: {avg_train_loss:.4f}\")\n\ntorch.save(model.state_dict(),'cnn_lstm.pth')\nprint(\"Model saved successfully.\")\n\nmodel = CNN_LSTM(input_size=7, hidden_size=1, output_size=1, dropout_rate=0.5).to(device)\nmodel.load_state_dict(torch.load('cnn_lstm.pth'))\nmodel.eval()\ny_pred = []\ny_true = []\ntest_loss = 0.0\nwith torch.no_grad():\n    for X_batch, y_batch in test_loader:\n        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n        output = model(X_batch)\n        loss = criterion(output, y_batch)\n        test_loss += loss.item() * X_batch.size(0)\n        y_pred.append(output.cpu().numpy())\n        y_true.append(y_batch.cpu().numpy())\ntest_loss /= len(test_loader)\nprint(test_loss)\n\ny_pred = np.concatenate(y_pred, axis=0)\ny_true = np.concatenate(y_true, axis=0)\n\ny_pred = scaler_y.inverse_transform(y_pred)\ny_true = scaler_y.inverse_transform(y_true)\n\nmse = mean_squared_error(y_true, y_pred)\nmae = mean_absolute_error(y_true, y_pred)\n\nprint(f\"Test MSE: {mse:.4f}\")\nprint(f\"Test MAE: {mae:.4f}\")\n\nplt.figure(figsize=(12, 6))\nplt.plot(y_true, label='True Value', color='blue')\nplt.plot(y_pred, label='Predicted Value', color='red', linestyle='--')\nplt.title('True vs Predicted')\nplt.xlabel('Time (Samples)')\nplt.ylabel('Value')\nplt.legend()\nplt.savefig('ETTh1.jpg')\n\n\ndef predict_future(model, last_data, future_steps, scaler_y, device):\n    model.eval()\n    predictions = []\n    current_input = torch.tensor(last_data, dtype=torch.float32).unsqueeze(0).to(\n        device)  # shape (1, time_step, features)\n\n    with torch.no_grad():\n        for _ in range(future_steps):\n            output = model(current_input)\n            predicted_value = output.cpu().numpy()  # shape (1, 1)\n            predictions.append(predicted_value)\n\n            predicted_tensor = torch.tensor(predicted_value, dtype=torch.float32).unsqueeze(0).repeat(1, 1,\n                                                                                                      current_input.size(-1)).to(device)\n            current_input = torch.cat((current_input[:, 1:, :], predicted_tensor), dim=1)\n\n    predictions = np.array(predictions).squeeze()  # \u53bb\u6389\u591a\u4f59\u7684\u7ef4\u5ea6\n    return scaler_y.inverse_transform(predictions)  # \u8f6c\u6362\u56de\u539f\u59cb\u7684\u5c3a\u5ea6\n\nX_test_scaled = test_dataset.X.cpu().numpy()\nfuture_steps = 10\nlast_test_data = X_test_scaled[-1]  # \u6700\u540e\u4e00\u6761\u6d4b\u8bd5\u6570\u636e\u4f5c\u4e3a\u8d77\u70b9\npredicted_future = predict_future(model, last_test_data, future_steps, scaler_y, device)\n\nplt.figure(figsize=(12, 6))\n\n# \u7ed8\u5236\u5386\u53f2\u503c\nn = 100  # \u663e\u793a\u771f\u5b9e\u503c\u7684\u6700\u540en\u4e2a\u70b9\nplt.plot(np.arange(len(y_true) - n, len(y_true)), y_true[-n:], label='True Value', color='blue')\nplt.plot(np.arange(len(y_true), len(y_true) + len(predicted_future)), predicted_future, label='Predicted Future', color='red', linestyle='--')\nplt.title('True vs Predicted (Future)')\nplt.xlabel('Time (Samples)')\nplt.ylabel('Value')\nplt.legend()\nplt.savefig('ETTh1_partial_true_future.jpg')\n</code></pre>"},{"location":"DeepL/Pytorch/view/","title":"<code>view</code> \u548c <code>reshape</code>","text":"<p><code>view</code> \u548c <code>reshape</code> \u662f PyTorch \u4e2d\u5e38\u7528\u7684\u5f20\u91cf\u53d8\u5f62\u65b9\u6cd5\uff0c\u4e24\u8005\u770b\u4f3c\u76f8\u4f3c\uff0c\u4f46\u5728\u5185\u5b58\u5904\u7406\u548c\u4f7f\u7528\u9650\u5236\u4e0a\u6709\u4e00\u4e9b\u533a\u522b\u3002\u4e0b\u9762\u89e3\u91ca\u5b83\u4eec\u7684\u533a\u522b\u5e76\u901a\u8fc7\u4f8b\u5b50\u5c55\u793a\u7528\u6cd5\u3002</p>"},{"location":"DeepL/Pytorch/view/#contiguous","title":"<code>contiguous</code>","text":"Python<pre><code>if self.mix:\n    out = out.transpose(2, 1).contiguous()\nout = out.view(B, L, -1)\n</code></pre> <p>\u5728 PyTorch \u4e2d\uff0cview \u64cd\u4f5c\u8981\u6c42\u5f20\u91cf\u5728\u5185\u5b58\u4e2d\u662f\u8fde\u7eed\u7684\u3002\u7531\u4e8e transpose \u5f80\u5f80\u4f1a\u5bfc\u81f4\u5f20\u91cf\u53d8\u6210\u975e\u8fde\u7eed\u7684\u5e03\u5c40\uff0c\u5fc5\u987b\u5728 view \u4e4b\u524d\u8c03\u7528 contiguous() \u6765\u4fdd\u8bc1\u6570\u636e\u7684\u8fde\u7eed\u6027\uff0c\u4ee5\u907f\u514d\u62a5\u9519\u3002</p>"},{"location":"DeepL/Pytorch/view/#view","title":"<code>view</code>","text":"<ul> <li>\u4f5c\u7528\uff1a<code>view</code> \u662f\u5bf9\u5f20\u91cf\u8fdb\u884c\u91cd\u65b0\u5f62\u72b6\u8c03\u6574\uff0c\u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf\uff0c\u4f46\u5b83\u5171\u4eab\u539f\u59cb\u5f20\u91cf\u7684\u5185\u5b58\u3002</li> <li>\u8981\u6c42\uff1a\u7531\u4e8e <code>view</code> \u5171\u4eab\u5185\u5b58\uff0c\u5b83\u8981\u6c42\u539f\u5f20\u91cf\u5728\u5185\u5b58\u4e2d\u662f\u8fde\u7eed\u7684\u3002\u4f7f\u7528 <code>view</code> \u4e4b\u524d\uff0c\u53ef\u4ee5\u8c03\u7528 <code>.contiguous()</code> \u65b9\u6cd5\u786e\u4fdd\u8fde\u7eed\u6027\u3002</li> <li>\u4f18\u70b9\uff1a\u4e0d\u4f1a\u521b\u5efa\u65b0\u7684\u5185\u5b58\u7a7a\u95f4\uff0c\u9002\u7528\u4e8e\u9700\u8981\u9ad8\u6548\u5b58\u50a8\u7684\u60c5\u51b5\u3002</li> </ul> <p>\u793a\u4f8b\uff1a</p> Python<pre><code>import torch\n\n# \u521b\u5efa\u4e00\u4e2a\u5f20\u91cf\ntensor = torch.tensor([[1, 2, 3], [4, 5, 6]])\n\n# \u4f7f\u7528 view \u6539\u53d8\u5f62\u72b6\ntensor_view = tensor.contiguous().view(3, 2)\n\nprint(\"Original tensor:\\n\", tensor)\nprint(\"View tensor:\\n\", tensor_view)\n\n# \u4fee\u6539 view \u540e\u539f\u59cb\u5f20\u91cf\u4e5f\u4f1a\u6539\u53d8\ntensor_view[0, 0] = 10\nprint(\"\\nAfter modifying view:\")\nprint(\"Original tensor:\\n\", tensor)\nprint(\"View tensor:\\n\", tensor_view)\n</code></pre>"},{"location":"DeepL/Pytorch/view/#reshape","title":"<code>reshape</code>","text":"<ul> <li>\u4f5c\u7528\uff1a<code>reshape</code> \u4e5f\u7528\u4e8e\u6539\u53d8\u5f20\u91cf\u7684\u5f62\u72b6\uff0c\u4f46\u4e0e <code>view</code> \u4e0d\u540c\uff0c\u5b83\u4f1a\u5c1d\u8bd5\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u5185\u5b58\u7a7a\u95f4\uff0c\u4e14\u4e0d\u8981\u6c42\u8f93\u5165\u5f20\u91cf\u5728\u5185\u5b58\u4e2d\u662f\u8fde\u7eed\u7684\u3002</li> <li>\u7075\u6d3b\u6027\uff1a<code>reshape</code> \u4f1a\u6839\u636e\u9700\u8981\u5728\u975e\u8fde\u7eed\u5185\u5b58\u7684\u60c5\u51b5\u4e0b\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf\u3002\u82e5\u539f\u59cb\u5f20\u91cf\u662f\u8fde\u7eed\u7684\uff0c<code>reshape</code> \u901a\u5e38\u4f1a\u76f4\u63a5\u8fd4\u56de\u5171\u4eab\u5185\u5b58\u7684\u65b0\u5f62\u72b6\u5f20\u91cf\uff08\u4e0e <code>view</code> \u7c7b\u4f3c\uff09\uff1b\u5426\u5219\u4f1a\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u526f\u672c\u3002</li> <li>\u9002\u7528\u573a\u666f\uff1a\u5bf9\u4e8e\u5185\u5b58\u975e\u8fde\u7eed\u7684\u5f20\u91cf\uff0c\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 <code>reshape</code> \u800c\u4e0d\u9700\u8981\u624b\u52a8\u8c03\u7528 <code>.contiguous()</code>\u3002</li> </ul> <p>\u793a\u4f8b\uff1a</p> Python<pre><code>import torch\n\n# \u521b\u5efa\u4e00\u4e2a\u5f20\u91cf\ntensor = torch.tensor([[1, 2, 3], [4, 5, 6]])\n\n# \u4f7f\u7528 reshape \u6539\u53d8\u5f62\u72b6\ntensor_reshape = tensor.reshape(3, 2)\n\nprint(\"Original tensor:\\n\", tensor)\nprint(\"Reshape tensor:\\n\", tensor_reshape)\n\n# \u4fee\u6539 reshape \u540e\u539f\u59cb\u5f20\u91cf\u4e5f\u4f1a\u6539\u53d8\uff08\u82e5\u6ca1\u6709\u521b\u5efa\u65b0\u7684\u5185\u5b58\u7a7a\u95f4\uff09\ntensor_reshape[0, 0] = 20\nprint(\"\\nAfter modifying reshape:\")\nprint(\"Original tensor:\\n\", tensor)\nprint(\"Reshape tensor:\\n\", tensor_reshape)\n</code></pre>"},{"location":"TEST/","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"TEST/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"TEST/#project-layout","title":"Project layout","text":"Text Only<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"}]}